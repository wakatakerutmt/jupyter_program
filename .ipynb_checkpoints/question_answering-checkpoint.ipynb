{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#項構造抽出\n",
    "def term_extractor(fileobject):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    import numpy as np\n",
    "    import re\n",
    "    import pprint\n",
    "    \n",
    "    def treatment(text):\n",
    "        text=re.sub(r'/.*?:', r':', text)\n",
    "        text=re.sub(r'(名|動|形|判){1}([0-9]+){1}', r'\\1', text) \n",
    "        text=re.sub(r'/([0-9])+(;|>)', r'\\2', text)\n",
    "        text=re.sub(r'/[A-Z]/', r'/', text)\n",
    "        text=re.sub(r'(:|;)(修飾|時間)/.*?;', r'\\1', text)\n",
    "        text=re.sub(r'(:|;)(修飾|時間)/.*?>', '>', text)\n",
    "        text=text.replace(r'//', r'/')\n",
    "        return text\n",
    "    \n",
    "    def select_term_structure():\n",
    "        all_res=fileobject.all()\n",
    "        fstring_l=all_res.split('\\n')\n",
    "        term_l=[]\n",
    "        for line in fstring_l:\n",
    "            term=''\n",
    "            if not re.findall(r'<態:受動.*?>', line) == []:\n",
    "                term+=''.join(re.findall(r'<態:受動.*?>', line))\n",
    "            if not re.findall(r'<項構造.*?>', line) == []:\n",
    "                term+=''.join(re.findall(r'<項構造:.*?>', line))\n",
    "            if term!='':\n",
    "                term_l.append(term)\n",
    "        return term_l\n",
    "\n",
    "    term_l=select_term_structure()\n",
    "    for i, term in enumerate(term_l):\n",
    "        term_l[i]=treatment(term)\n",
    "    #pprint.pprint(term_l)\n",
    "    return term_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#項の数でフィルタリング\n",
    "def term_inspection(term_l):\n",
    "    import pprint\n",
    "    new_l=[]\n",
    "    for term in term_l:\n",
    "        if term.find(r'/判:') != -1:\n",
    "            new_l.append(term)\n",
    "        else:\n",
    "            if term[len('<項構造:')+1:].count(':')>=2:\n",
    "                new_l.append(term)\n",
    "    return new_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#項構造がとれないとき、係り受け構造からトリプルを取得ー＞うーん\n",
    "def term_dependency(text):\n",
    "    import pprint\n",
    "    from pyknp import KNP\n",
    "    import re\n",
    "    def select_nrml_rep(fstring):\n",
    "        bgn = fstring.find('正規化代表表記:')\n",
    "        end = fstring.find('>', bgn + 1)\n",
    "        if bgn < 0:\n",
    "            if fstring.find(r'<ID:（文末）>')>=0:\n",
    "                return '?'\n",
    "        string=''.join(re.findall(r'<正規化代表表記:.*?/', fstring)).replace(r'<正規化代表表記:', '').strip(r'/')\n",
    "        s=fstring.find(r'/', bgn+1, end)\n",
    "        p=fstring.find(r'\\+', s)\n",
    "        for i in range(bgn, end):\n",
    "            if p >= 0:\n",
    "                s=fstring.find(r'/', p, end)\n",
    "                string+=fstring[p+1:s]\n",
    "                p=fstring.find(r'/', s, end)\n",
    "            else:\n",
    "                break\n",
    "        return string\n",
    "\n",
    "    if text.find('?') < 0:\n",
    "        text+='?'\n",
    "    knp=KNP(option='-tab -anaphora', jumanpp=True)\n",
    "    result=knp.parse(text)\n",
    "    bnst_list=result.bnst_list()\n",
    "    bnst_dic = dict((x.bnst_id, x) for x in bnst_list)\n",
    "\n",
    "    tuples = []\n",
    "    for bnst in bnst_list:\n",
    "        if bnst.parent_id != -1:\n",
    "            # (from, to)\n",
    "            tuples.append((select_nrml_rep(bnst.fstring), select_nrml_rep(bnst_dic[bnst.parent_id].fstring)))\n",
    "    rel=''\n",
    "    for t in tuples:\n",
    "        pprint.pprint(t[0]+'=>'+t[1])\n",
    "        if t[1]=='?':\n",
    "            rel=t[0]\n",
    "    term_l=[]\n",
    "    for t in tuples:\n",
    "        if t[1]==rel:\n",
    "            term_l.append(t[0])\n",
    "    pprint.pprint(term_l)\n",
    "    tmp=''\n",
    "    tmp2=''\n",
    "    _list=result.all().split('\\n')\n",
    "    for line in _list:\n",
    "        if line.find(rel) >= 0:\n",
    "            if not re.findall(r'<態:受動.*?>', line) == []:\n",
    "                tmp2+=''.join(re.findall(r'<態:受動.*?>', line))\n",
    "        kaku=line.find('<解析格:')\n",
    "        if kaku >= 0:\n",
    "            rep=select_nrml_rep(line)\n",
    "            if rep in term_l:\n",
    "                tmp+=line[kaku+len('<解析格:'):line.find('>', kaku+1)]+'/'+rep+';'   \n",
    "    term=tmp2+'<項構造:'+rel+':動:'+tmp[:-1]+'>'\n",
    "    #<項構造:hoge:格/foo;格/foo>\n",
    "    return term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make tripe\n",
    "def tuple_maker(term_list, _input):\n",
    "    import numpy as np\n",
    "    import re\n",
    "    \n",
    "    def triple_extract(flag_l, rel, word_l, kaku_l):\n",
    "        def pick_word(pair_l):\n",
    "            for i, p in enumerate(pair_l):\n",
    "                if p in kaku_l:\n",
    "                    return word_l[kaku_l.index(p)]\n",
    "            return ''\n",
    "\n",
    "        tri=['']*3\n",
    "        pair_ga=['ヲ', 'ニ', 'ト', 'デ', 'カラ', 'ガ２','ニツク', 'ニタイスル'\n",
    "                 , 'ニヨル', 'ニオク', 'ニカンスル']\n",
    "        pair_toha=[i for i in pair_ga if i!='ト']\n",
    "        \n",
    "        if flag_l[0]==0:\n",
    "            if rel=='有る':\n",
    "                tri[1]='rdf:subClassOf'\n",
    "            else:\n",
    "                tri[1]=rel\n",
    "            for i, kaku in enumerate(kaku_l):\n",
    "                if kaku=='ガ':\n",
    "                    tri[0]=word_l[i]\n",
    "                    tri[2]=pick_word(pair_ga)\n",
    "                elif kaku=='ト' and 'とは' in _input:\n",
    "                    tri[0]=word_l[i]\n",
    "                    tri[2]=pick_word(pair_toha)\n",
    "        else:#判定詞\n",
    "            tri[1]='rdf:subClassOf'\n",
    "            tri[2]=rel\n",
    "            for i, kaku in enumerate(kaku_l):\n",
    "                if kaku=='ガ':\n",
    "                    tri[0]=word_l[i]\n",
    "                elif kaku=='ト' and 'とは' in _input:\n",
    "                    tri[0]=word_l[i]\n",
    "        if tri[0]=='':#主格相当のものがないとき\n",
    "            for i, kaku in enumerate(kaku_l):\n",
    "                if kaku in pair_ga:\n",
    "                    tri[2]=word_l[i]\n",
    "        if flag_l[1]==1:#受身\n",
    "            tmp=''\n",
    "            tmp=tri[2]\n",
    "            tri[2]=tri[0]\n",
    "            tri[0]=tmp\n",
    "        return tri        \n",
    "        \n",
    "    def flag_set(text):\n",
    "        flag=[0]*2  #[is_a, rev]\n",
    "        if text.find(r'<態:受動') >= 0:\n",
    "            flag[1]=1\n",
    "        if text.find(r'判:') >= 0:\n",
    "            flag[0]=1\n",
    "        return flag\n",
    "    \n",
    "    def split(st):\n",
    "        #rel\n",
    "        rel=''.join(re.findall(r'<項構造:(.*?):', st)).replace('<項構造:', '').strip(':')\n",
    "        case=[]\n",
    "        word=[]\n",
    "        #case\n",
    "        bgn=st.find(r':', st.find(rel)+len(rel)+1)\n",
    "        for i in range(len(st)):\n",
    "            if bgn>=0:\n",
    "                slash=st.find(r'/', bgn)\n",
    "                case.append(st[bgn+1:slash])\n",
    "                bgn=st.find(r';', slash)\n",
    "            else:\n",
    "                break\n",
    "        #word\n",
    "        end=0\n",
    "        for i in range(len(st)):\n",
    "            bgn=st.find(r'/', end)\n",
    "            if bgn>=0:\n",
    "                end=st.find(r';', bgn)\n",
    "                if end < 0:\n",
    "                    end=st.find(r'>', bgn)\n",
    "                word.append(st[bgn+1:end])\n",
    "            else:\n",
    "                break    \n",
    "        return [rel, case, word]\n",
    "    \n",
    "    flag_list=[]\n",
    "    kaku_list=[]\n",
    "    word_list=[]\n",
    "    relation_list=[]\n",
    "    for term in term_list:\n",
    "        [x,y,z]=split(term)\n",
    "        flag_list.append(flag_set(term))\n",
    "        kaku_list.append(y)\n",
    "        word_list.append(z)\n",
    "        relation_list.append(x)\n",
    "    print(flag_list)\n",
    "    print(kaku_list)\n",
    "    print(word_list)\n",
    "    print(relation_list)\n",
    "    tuple_l=[]\n",
    "    for i in range(len(term_list)):\n",
    "        tuple_l.append(triple_extract(flag_list[i], relation_list[i]\n",
    "                                      , word_list[i], kaku_list[i]))\n",
    "    return tuple_l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#複数とれている場合トリプルからもっとも重要なものを選ぶ->ok\n",
    "def choose_tuple(_list, object):\n",
    "    import numpy as np\n",
    "    import re\n",
    "    def wakati():\n",
    "        mrph_list=object.mrph_list()\n",
    "        wakati=''\n",
    "        for mrph in mrph_list:\n",
    "            hinsi = mrph.hinsi\n",
    "            if hinsi in  ['名詞', '未定義語'] and mrph.bunrui not in ['形式名詞']:  # 対象とする品詞\n",
    "                word = mrph.midasi\n",
    "                if len(wakati)==0:\n",
    "                    wakati=word\n",
    "                else:\n",
    "                    wakati=wakati + ' ' + word            \n",
    "        return wakati\n",
    "    def tf_idf(_wakati):\n",
    "        import pickle\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        word_score=[]#[[score, word], [score, word],,,]\n",
    "        with open(r'./pickle/textbook_wakati_n.pickle','rb') as fileobject:\n",
    "            doc_list=pickle.load(fileobject)\n",
    "        doc_list.append(_wakati)\n",
    "\n",
    "        vectorizer=TfidfVectorizer(use_idf=True, token_pattern=u'(?u)\\\\b\\\\w+\\\\b')\n",
    "        vecs=vectorizer.fit_transform(doc_list)\n",
    "        index = vecs.toarray().argsort(axis=1)[:,::-1]\n",
    "        feature_names = np.array(vectorizer.get_feature_names())\n",
    "        feature_words = feature_names[index]\n",
    "        del doc_list\n",
    "        \n",
    "        for j in range(0,len(_wakati.split(' '))):\n",
    "            word_score.append([vecs.toarray()[len(feature_words)-1]\n",
    "                                   [index[len(feature_words)-1][j]],\n",
    "                                  feature_words[-1:][0][j]])    \n",
    "        return word_score\n",
    "    \n",
    "    def scoring(word_score_l):\n",
    "        _tuple_l=_list[:]\n",
    "        _tuple_score_l=[]\n",
    "        index=0\n",
    "        for i in range(len(_tuple_l)):\n",
    "            score=0\n",
    "            for j in range(len(_tuple_l[i])):\n",
    "                for word_score in word_score_l:\n",
    "                    if _tuple_l[i][j].upper().find(word_score[1].upper()) >=0:\n",
    "                        score+=word_score[0]\n",
    "            _tuple_score_l.append(score)\n",
    "            print(_tuple_score_l)\n",
    "        tmp=.0\n",
    "        for i, scr in enumerate(_tuple_score_l):\n",
    "            if scr>=tmp:\n",
    "                tmp=scr\n",
    "                index=i\n",
    "        return index\n",
    "    \n",
    "#        return _tuple_score_l.index(max(_tuple_score_l))\n",
    "    if len(_list)==1:\n",
    "        return _list[0]\n",
    "    else:\n",
    "        return _list[scoring(tf_idf(wakati()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#質問文と疑問詞、キーワードから質問タイプに分類するー＞残りバイアス調整\n",
    "#分類先：5W1Hから不要なものを除く ok:(why, how, (where and when), what)\n",
    "def question_type(intr, _input_q, triple):\n",
    "    import pickle\n",
    "    from gensim import models\n",
    "    import numpy as np\n",
    "    import pprint\n",
    "    import warnings\n",
    "    \n",
    "    def min_max(x, axis=None):\n",
    "        min = x.min(axis=axis, keepdims=True)\n",
    "        max = x.max(axis=axis, keepdims=True)\n",
    "        result = (x-min)/(max-min)\n",
    "        return result\n",
    "    \n",
    "    def zscore(x, axis = None):#正規化、標準偏差1, 平均0\n",
    "        xmean = x.mean(axis=axis, keepdims=True)\n",
    "        xstd  = np.std(x, axis=axis, keepdims=True)\n",
    "        return (x-xmean)/xstd\n",
    "    def split_text(text):\n",
    "        from pyknp import Jumanpp\n",
    "        flag=0 #複合語検出用\n",
    "        jumanpp = Jumanpp()\n",
    "        result = jumanpp.analysis(text)\n",
    "        st_wakati=''\n",
    "        for mrph in result.mrph_list():\n",
    "            hinsi = mrph.hinsi\n",
    "            word = mrph.midasi\n",
    "            if flag == 0:\n",
    "                if st_wakati=='':\n",
    "                    st_wakati=word\n",
    "                else:\n",
    "                    st_wakati+=' '+ word\n",
    "            elif flag == 1 and hinsi=='名詞':#単語が名詞で前の単語も名詞\n",
    "                st_wakati+=word\n",
    "            if hinsi in [\"名詞\"] and ('時相' not in mrph.bunrui and '数' not in mrph.bunrui):\n",
    "                flag=1\n",
    "            else:\n",
    "                flag=0\n",
    "        return st_wakati\n",
    "    \n",
    "    scr_typ=np.zeros(5)\n",
    "    scr_sent=np.zeros(5)\n",
    "    scr_intr=np.zeros(5)\n",
    "    tmpl_sent=['理由', '方法', '場面', '時', '意味']\n",
    "    tmpl_intr=['何故', 'どう', 'どこ', 'いつ', '何']\n",
    "    \n",
    "    with open(r'./pickle/doc_wiki_model_neologd_300.pickle', 'rb') as f:\n",
    "        doc2vec_model=pickle.load(f)\n",
    "    s=_input_q\n",
    "    if triple[0]!='' and triple[0]!='何' and triple[0] not in tmpl_sent:\n",
    "        s=s.replace(triple[0], 'e')\n",
    "    if triple[2]!='' and triple[2]!='何' and triple[2] not in tmpl_sent:\n",
    "        s=s.replace(triple[2], 'e')\n",
    "    \n",
    "    warnings.filterwarnings('ignore')\n",
    "    for i, tmpl in enumerate(tmpl_sent):\n",
    "        scr_sent[i]=(doc2vec_model.docvecs.similarity_unseen_docs(\n",
    "            doc2vec_model, split_text(tmpl), split_text(s), \n",
    "             steps=1000))\n",
    "    pprint.pprint(scr_sent)\n",
    "    pprint.pprint(f'normalization:{min_max(scr_sent)}')\n",
    "    del doc2vec_model\n",
    "    if intr != '':\n",
    "        with open(r'./pickle/word2vec_model.pickle', 'rb') as f:\n",
    "            word2vec_model=pickle.load(f)\n",
    "        for i,tmpl in enumerate(tmpl_intr):\n",
    "            scr_intr[i]=(word2vec_model.similarity( \n",
    "                split_text(intr).split(' ')[0],tmpl)\n",
    "        )\n",
    "        pprint.pprint(scr_intr)\n",
    "        pprint.pprint(f'normalization:{min_max(scr_intr)}')\n",
    "        del word2vec_model\n",
    "        #標準化z-score normalization or min-max normalization\n",
    "        if scr_intr.argmax() != scr_sent.argmax() and scr_intr.argmax() == 4:\n",
    "            #pprint.pprint('6:4')\n",
    "            pprint.pprint('バイアス付与')\n",
    "            scr_typ=0.7*min_max(scr_sent)+0.3*min_max(scr_intr)\n",
    "            #scr_typ=0.6*zscore(scr_sent)+0.4*zscore(scr_intr)\n",
    "        elif scr_intr[scr_intr.argmax()]==1:\n",
    "            pprint.pprint('バイアス付与')\n",
    "            scr_typ=0.4*min_max(scr_sent)+0.6*min_max(scr_intr)\n",
    "            #scr_typ=0.4*zscore(scr_sent)+0.6*zscore(scr_intr)\n",
    "        else:\n",
    "            scr_typ=min_max(scr_sent)+min_max(scr_intr)\n",
    "            #scr_typ=zscore(scr_sent)+zscore(scr_intr)\n",
    "        pprint.pprint(f'正規化類似度{scr_typ}')\n",
    "    else:\n",
    "        scr_typ=scr_sent\n",
    "    return scr_typ.argmax()\n",
    "#q_type: 0-why, 1-how, 2-when and where, 3-what"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#トリプルと質問タイプからsparqlの決定->思うことは書いたok\n",
    "def position(triple):\n",
    "        posi=[0]*3\n",
    "        for i in range(3):\n",
    "            if triple[i]!='' and not (triple[i]=='何' or triple[i]=='なに' or triple[i]=='する'):\n",
    "                posi[i]=1\n",
    "        return posi\n",
    "    \n",
    "def make_sparql(qtype, triple):\n",
    "    import pprint\n",
    "    posi=position(triple)\n",
    "    limit_expr=''\n",
    "    target=''\n",
    "    condition=''\n",
    "    \n",
    "    sro=['?s', '?r', '?o']\n",
    "    for i, p in enumerate(posi):\n",
    "        if p==0:\n",
    "            target+=sro[i] + ' '\n",
    "            condition+=sro[i] + ' '\n",
    "        else:\n",
    "            if i==1:\n",
    "                condition+=f'link:{triple[i]} '\n",
    "            else:\n",
    "                condition+=f'local:{triple[i]} '\n",
    "    pprint.pprint(target)\n",
    "    pprint.pprint(condition)\n",
    "    if posi.count(1)==3: #同義：target=='':#質問文でトリプルが埋まっている(もはやyes/no)\n",
    "        target=r'*'\n",
    "        condition=f'''\n",
    "        {{\n",
    "        ?s ?r ?o .\n",
    "        filter ((?s=local:{triple[0]}) &&(?r=link:{triple[1]}) && (?o=local:{triple[2]}))\n",
    "        }}\n",
    "        union\n",
    "        {{\n",
    "        local:{triple[0]} link:of ?of .\n",
    "        }}\n",
    "        '''\n",
    "    else:\n",
    "        condition+=' .'\n",
    "        if posi[0]==0:\n",
    "            tmp=''\n",
    "            if posi[1]==1 and posi[2]==1:\n",
    "                tmp=f'''\n",
    "                ?s link:{triple[1]} ?b.\n",
    "                ?b ?x local:{triple[2]}.\n",
    "                '''\n",
    "            elif posi[1]==1 and posi[2]==0:\n",
    "                tmp=f'''\n",
    "                ?s link:{triple[1]} ?b.\n",
    "                ?b ?x ?o.\n",
    "                '''\n",
    "            elif posi[1]==0 and posi[2]==1:\n",
    "                tmp=f'''\n",
    "                ?s ?r ?b.\n",
    "                ?b ?x local:{triple[2]}.\n",
    "                '''\n",
    "            else:#何もないからここは通らない\n",
    "                tmp=f'''\n",
    "                ?s ?r ?b.\n",
    "                ?b ?x ?o.\n",
    "                '''\n",
    "            target+=' ?b'\n",
    "            condition=f'''\n",
    "            {{\n",
    "                {condition}\n",
    "                optional{{\n",
    "                    ?b ?x ?s.\n",
    "                    filter(isBlank(?s) && regex(str(?x), str(rdf:_)))\n",
    "                }}\n",
    "            }}\n",
    "            union{{\n",
    "                {condition }\n",
    "            }}\n",
    "            union{{\n",
    "                {tmp}\n",
    "                filter(isBlank(?b) && regex(str(?r), str(link:)) && regex(str(?x), str(rdf:_)))\n",
    "            }}\n",
    "            '''\n",
    "        else:\n",
    "            tmp=''\n",
    "            if posi[1]==1 and posi[2]==1:\n",
    "                tmp=f'''\n",
    "                ?b link:{triple[1]} local:{triple[2]}.\n",
    "                ?b ?x local:{triple[0]}.\n",
    "                '''\n",
    "            elif posi[1]==1 and posi[2]==0:\n",
    "                tmp=f'''\n",
    "                ?b link:{triple[1]} ?o.\n",
    "                ?b ?x local:{triple[0]}.\n",
    "                '''\n",
    "            elif posi[1]==0 and posi[2]==1:\n",
    "                tmp=f'''\n",
    "                ?b ?r local:{triple[2]}.\n",
    "                ?b ?x local:{triple[0]}.\n",
    "                '''\n",
    "            else:#何もないからここは通らない\n",
    "                tmp=f'''\n",
    "                ?b ?r ?o.\n",
    "                ?b ?x local:{triple[0]}.\n",
    "                '''\n",
    "                \n",
    "            target+=' ?b'\n",
    "            x=condition\n",
    "            condition=f'''\n",
    "            {{\n",
    "                {condition }\n",
    "            }}\n",
    "            union{{\n",
    "                {tmp}\n",
    "                filter(isBlank(?b) && regex(str(?x), str(rdf:_)))\n",
    "            }}\n",
    "            '''\n",
    "            if posi[2]==0:\n",
    "                condition+=f'''\n",
    "                union{{\n",
    "                {condition}\n",
    "                optional{{\n",
    "                ?o ?x ?b.\n",
    "                filter(isBlank(?o) && regex(str(?x), str(rdf:_)))\n",
    "                }}\n",
    "                }}\n",
    "                '''\n",
    "#            if not (posi[1]==1 and posi[2]==1)==True:#**********\n",
    " #               condition+=f'''\n",
    "  #               union{{\n",
    "   #             {condition}\n",
    "    #            optional{{\n",
    "     #               ?o ?x ?b.\n",
    "      #              filter(isBlank(?o) && regex(str(?x), str(rdf:_)))\n",
    "       #             }}\n",
    "        #        }}\n",
    "         #       '''\n",
    "            target+=' ?of ?bof'\n",
    "            condition=f'''\n",
    "                {{\n",
    "                {condition} \n",
    "                }}\n",
    "                union\n",
    "                {{\n",
    "                local:{triple[0]} link:of ?of .\n",
    "                optional{{\n",
    "                ?of ?y ?bof.\n",
    "                filter(isBlank(?of))\n",
    "                }}\n",
    "                }}\n",
    "            '''\n",
    "    query=f'''\n",
    "    PREFIX link: <http://localhost:3030/link/>\n",
    "    PREFIX local: <http://localhost:3030/>\n",
    "    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    select distinct {target} \n",
    "    where {{\n",
    "        {{\n",
    "        {condition}\n",
    "        }}\n",
    "    }}\n",
    "    LIMIT 50\n",
    "    '''\n",
    "    return query.replace('\\r', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#トリプルと質問タイプからsparqlの決定->思うことは書いたok\n",
    "def position(triple):\n",
    "        posi=[0]*3\n",
    "        for i in range(3):\n",
    "            if triple[i]!='' and not (triple[i]=='何' or triple[i]=='なに' or triple[i]=='する'):\n",
    "                posi[i]=1\n",
    "        return posi\n",
    "    \n",
    "def make_sparql(qtype, triple):\n",
    "    \n",
    "    posi=position(triple)\n",
    "    limit_expr=''\n",
    "    target=''\n",
    "    condition=''\n",
    "    sro=['?s', '?r', '?o']\n",
    "\n",
    "    if posi.count(1)==3:\n",
    "        target=r'*'\n",
    "        condition=f'''\n",
    "        {{\n",
    "        ?s ?r ?o.\n",
    "        filter((?s=local:{triple[0]} || ?s=:\\#{triple[0]}) \n",
    "            && (?r=link:{triple[1]}) && (?o=local:{triple[2]} || ?o=:\\#{triple[2]}))\n",
    "        }}\n",
    "        '''\n",
    "    else:\n",
    "        for i, p in enumerate(posi):\n",
    "            if p==0:\n",
    "                target+=sro[i]+' '\n",
    "                condition+=sro[i]+' '\n",
    "            else:\n",
    "                if i==1:\n",
    "                    condition+=f'link:{triple[i]} '\n",
    "                else:\n",
    "                    condition+=f':\\#{triple[i]} '\n",
    "        condition+=' .'\n",
    "   \n",
    "        if posi[0]==0:\n",
    "            tmp=''\n",
    "            if posi[1]==1 and posi[2]==1:\n",
    "                tmp=f'''\n",
    "                ?s link:{triple[1]} ?b.\n",
    "                ?b ?x local:{triple[2]}.\n",
    "                '''\n",
    "            elif posi[1]==1 and posi[2]==0:\n",
    "                tmp=f'''\n",
    "                ?s link:{triple[1]} ?b.\n",
    "                ?b ?x ?o.\n",
    "                '''\n",
    "            elif posi[1]==0 and posi[2]==1:\n",
    "                tmp=f'''\n",
    "                ?s ?r ?b.\n",
    "                ?b ?x local:{triple[2]}.\n",
    "                '''\n",
    "            else:#何もないからここは通らない\n",
    "                tmp=f'''\n",
    "                ?s ?r ?b.\n",
    "                ?b ?x ?o.\n",
    "                '''\n",
    "            target+=' ?b'\n",
    "            condition=f'''\n",
    "            {{\n",
    "                {condition}\n",
    "                optional{{\n",
    "                    ?b ?x ?s.\n",
    "                    filter(isBlank(?s) && regex(str(?x), str(rdf:_)))\n",
    "                }}\n",
    "            }}\n",
    "            union{{\n",
    "                {condition }\n",
    "            }}\n",
    "            union{{\n",
    "                {tmp}\n",
    "                filter(isBlank(?b) && regex(str(?r), str(link:)) && regex(str(?x), str(rdf:_)))\n",
    "            }}\n",
    "            '''\n",
    "        else:\n",
    "            tmp=''\n",
    "            if posi[1]==1 and posi[2]==1:\n",
    "                tmp=f'''\n",
    "                ?b link:{triple[1]} local:{triple[2]}.\n",
    "                ?b ?x local:{triple[0]}.\n",
    "                '''\n",
    "            elif posi[1]==1 and posi[2]==0:\n",
    "                tmp=f'''\n",
    "                ?b link:{triple[1]} ?o.\n",
    "                ?b ?x local:{triple[0]}.\n",
    "                '''\n",
    "            elif posi[1]==0 and posi[2]==1:\n",
    "                tmp=f'''\n",
    "                ?b ?r local:{triple[2]}.\n",
    "                ?b ?x local:{triple[0]}.\n",
    "                '''\n",
    "            else:#何もないからここは通らない\n",
    "                tmp=f'''\n",
    "                ?b ?r ?o.\n",
    "                ?b ?x local:{triple[0]}.\n",
    "                '''\n",
    "                \n",
    "            target+=' ?b'\n",
    "            x=condition\n",
    "            condition=f'''\n",
    "            {{\n",
    "                {condition }\n",
    "            }}\n",
    "            union{{\n",
    "                {tmp}\n",
    "                filter(isBlank(?b) && regex(str(?x), str(rdf:_)))\n",
    "            }}\n",
    "            '''\n",
    "            if posi[2]==0:\n",
    "                condition+=f'''\n",
    "                union{{\n",
    "                {condition}\n",
    "                optional{{\n",
    "                ?o ?x ?b.\n",
    "                filter(isBlank(?o) && regex(str(?x), str(rdf:_)))\n",
    "                }}\n",
    "                }}\n",
    "                '''\n",
    "#            if not (posi[1]==1 and posi[2]==1)==True:#**********\n",
    " #               condition+=f'''\n",
    "  #               union{{\n",
    "   #             {condition}\n",
    "    #            optional{{\n",
    "     #               ?o ?x ?b.\n",
    "      #              filter(isBlank(?o) && regex(str(?x), str(rdf:_)))\n",
    "       #             }}\n",
    "        #        }}\n",
    "         #       '''\n",
    "            target+=' ?of ?bof'\n",
    "            condition=f'''\n",
    "                {{\n",
    "                {condition} \n",
    "                }}\n",
    "                union\n",
    "                {{\n",
    "                local:{triple[0]} link:of ?of .\n",
    "                optional{{\n",
    "                ?of ?y ?bof.\n",
    "                filter(isBlank(?of))\n",
    "                }}\n",
    "                }}\n",
    "            '''\n",
    "    query=f'''\n",
    "    PREFIX : <http://www.semanticweb.org/kojima/ontologies/v3/>\n",
    "    PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "    PREFIX link: <http://localhost:3030/link/>\n",
    "    PREFIX local: <http://localhost:3030/>\n",
    "    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    select distinct {target} \n",
    "    where {{\n",
    "        {{\n",
    "        {condition}\n",
    "        }}\n",
    "    }}\n",
    "    LIMIT 50\n",
    "    '''\n",
    "    return query.replace('\\r', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#クエリからRDFに問い合わせて結果を返す->ok\n",
    "def retrieval(q):\n",
    "    from SPARQLWrapper import SPARQLWrapper, CSV\n",
    "    import re\n",
    "    sparql=SPARQLWrapper(r'http://localhost:3030/data_base/query')\n",
    "    sparql.setQuery(q)\n",
    "    sparql.setReturnFormat(CSV)\n",
    "    results=sparql.query().convert()\n",
    "    rdfinfo=results.decode('UTF-8')\n",
    "    rdfinfo=rdfinfo.replace('\\r', '')\n",
    "    return rdfinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#情報欠損検出、といかえし->したいことは書いたおk\n",
    "def ask_back(target, keyword_arr, triple):\n",
    "    import re\n",
    "    from pyknp import KNP\n",
    "    import pprint\n",
    "    keyword=[]\n",
    "    add=[]\n",
    "    of_l=[]\n",
    "#問い返し生成\n",
    "    if 'of' in target:\n",
    "        of_l=util('of', 'bof', keyword_arr, target)\n",
    "        if len(of_l) >1: #複数のof\n",
    "            for word in of_l:\n",
    "                if type(word)==list:\n",
    "                    string=''\n",
    "                    for i, w in enumerate(word):\n",
    "                        if i<=len(word)-1:\n",
    "                            string+=w+'と'\n",
    "                        else:\n",
    "                            string+=w\n",
    "                    ans=input(f'{string}と{triple[0]}ですか?(はい or いいえ)：')\n",
    "                else:\n",
    "                    ans=input(f'{word}の{triple[0]}ですか?(はい or いいえ)：')\n",
    "                if ans=='はい':\n",
    "                    add.append(['of', word])\n",
    "                    break\n",
    "        elif len(of_l)==1:\n",
    "            add.append(['of', of_l[0]])\n",
    "            \n",
    "    posi=position(triple)\n",
    "    keyword=[]\n",
    "    t=[]\n",
    "    if 's' in target:\n",
    "        t.append('s')\n",
    "        keyword=util('s', 'b', keyword_arr, target)\n",
    "    elif 'o' in target:\n",
    "        t.append('o')\n",
    "        print('yyyyy')\n",
    "        keyword=util('o', 'b', keyword_arr, target)\n",
    "    elif 'r' in target:\n",
    "        t.append('r')\n",
    "        keyword=util('r', 'b', keyword_arr, target)\n",
    "    try:\n",
    "        keyword=[s for s in set(keyword) if s!='']\n",
    "    except TypeError:\n",
    "        keyword=keyword\n",
    "    print(t)\n",
    "    print(f'in ask_back:{keyword}')\n",
    "    print(posi)\n",
    "    if posi.count(0)>1 or len(keyword)>1:\n",
    "        if posi[0]==1:\n",
    "            l=[x for x in keyword if x]\n",
    "            if t[0]=='r':\n",
    "                for key in l:\n",
    "                    if type(key)==list:\n",
    "                        key=','.join(key)\n",
    "                    k=KNP(option='-tab -anaphora', jumanpp=True)\n",
    "                    res=k.parse(triple[1])\n",
    "                    bnst_l=res.bnst_list()\n",
    "                    for bnst in bnst_l:\n",
    "                        bnst_f=bnst.fstring\n",
    "                        if (bnst_f.find('<用言:動>')>=0 or bnst_f.find('<用言:形>')>=0):\n",
    "                            ans=input(f'{triple[0]}の{key}ものですか?(はい or いいえ)：')\n",
    "                        elif bnst_f.find('<用言:判>')>=0 and bnst_f.find('サ変')<0:\n",
    "                            ans=input(f'{triple[0]}の{key}のことですか?(はい or いいえ)：')\n",
    "                        else:\n",
    "                            ans=input(f'{triple[0]}の{key}ですか?(はい or いいえ)：')\n",
    "                    if ans=='はい':\n",
    "                        add.append([t[0], key])\n",
    "                        break\n",
    "            else:\n",
    "                for key in l:\n",
    "                    if type(key)==list:\n",
    "                        key=','.join(key)\n",
    "                    ans=input(f'{triple[0]}の{key}のことですか？(はい or いいえ)')\n",
    "                    if ans=='はい':\n",
    "                        add.append([t[0], key])\n",
    "                        break\n",
    "        elif posi[1]==1:\n",
    "            \n",
    "            l=[x for x in keyword if x]\n",
    "            for key in l:\n",
    "                if type(key)==list:\n",
    "                    key=','.join(key)\n",
    "                k=KNP(option='-tab -anaphora', jumanpp=True)\n",
    "                res=k.parse(triple[1])\n",
    "                bnst_l=res.bnst_list()\n",
    "                for bnst in bnst_l:\n",
    "                    bnst_f=bnst.fstring\n",
    "                    if bnst_f.find('<用言:動>')>=0 or bnst_f.find('<用言:形>')>=0:\n",
    "                        ans=input(f'{key}の{triple[1]}ものですか?(はい or いいえ)：')\n",
    "                    elif bnst_f.find('<用言:判>')>=0 and bnst_f.find('サ変')<0:\n",
    "                        ans=input(f'{key}のことですか?(はい or いいえ)：')\n",
    "                    else:\n",
    "                        ans=input(f'{key}の{triple[1]}ですか?(はい or いいえ)：')\n",
    "                if ans=='はい':\n",
    "                    add.append([t[0], key])\n",
    "                    break\n",
    "        elif posi[2]==1: #oのみ情報あり\n",
    "            l=[x for x in keyword if x]\n",
    "            for key in l:\n",
    "                if type(key)==list:\n",
    "                    key=','.join(key)\n",
    "                ans=input(f'{key}の{triple[2]}のことですか?(はい or いいえ)：')\n",
    "                if ans=='はい':\n",
    "                    add.append([t[0],key])\n",
    "                    break\n",
    "        else:    #all zero\n",
    "            raise Exception('Error@ask_back()')\n",
    "    elif len(keyword)==1:\n",
    "        print(t)\n",
    "        print(keyword)\n",
    "        add.append([t[0], keyword[0]])\n",
    "    pprint.pprint(add)\n",
    "    return add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#類似度の高いRDF内の関係を返す -> ok\n",
    "def rel_modi(triple):\n",
    "    from SPARQLWrapper import SPARQLWrapper, CSV\n",
    "    import pickle\n",
    "    import re\n",
    "    import gensim.models       \n",
    "    with open(r'./pickle/word2vec_wiki_model.pickle', 'rb') as f:\n",
    "        word2vec_model=pickle.load(f)    \n",
    "    target='?r'\n",
    "    condition=''\n",
    "\n",
    "    posi=position(triple)\n",
    "    print(posi)\n",
    "    if posi[0]!=0 and posi[2]!=0:\n",
    "        condition+=f'local:{triple[0]} ?r local:{triple[2]} .'\n",
    "    elif posi[0]!=0:\n",
    "        condition+=f'local:{triple[0]} ?r ?o .'\n",
    "    elif posi[2]!=0:\n",
    "        condition+=f'?s ?r local:{triple[2]} .'\n",
    "    else:\n",
    "        condition+=f'?s ?r ?o .'\n",
    "    print(condition)\n",
    "    query=f'''\n",
    "    PREFIX link: <http://localhost:3030/link/>\n",
    "    PREFIX local: <http://localhost:3030/>\n",
    "    select distinct {target}\n",
    "    where{{\n",
    "     {condition}\n",
    "    }}\n",
    "    '''\n",
    "    rdfinfo=retrieval(query)\n",
    "    rdf_l=rdfinfo.split('\\n')\n",
    "    rel_s=[]\n",
    "    rdf_l.pop(0) #r\n",
    "    for i,rdf in enumerate(rdf_l):\n",
    "        rdf_l[i]=re.sub(r'http://localhost:3030(/|link/)*', '', rdf_l[i])\n",
    "        rdf_l[i]=re.sub(r'http://www.w3.org/2000/01/rdf-schema#(label|Class)', '', rdf_l[i])\n",
    "        rdf_l[i]=re.sub(r'http://www.w3.org/1999/02/22-rdf-syntax-ns#.*', '', rdf_l[i])\n",
    "        rdf_l[i]=re.sub(r'http://www.semanticweb.org/kojima/ontologies/v3/#', '', rdf_l[i])\n",
    "    rdf_l=[x for x in rdf_l if x]\n",
    "    print(rdf_l)\n",
    "    if len(rdf_l)==1:#そもそも類似可能性語がひとつしかない\n",
    "        return rdf_l[0]\n",
    "    else:\n",
    "        import warnings\n",
    "        warnings.filterwarnings('ignore')\n",
    "        for rdf in rdf_l:\n",
    "            if rdf in word2vec_model.wv and triple[1] in word2vec_model.wv:\n",
    "                rel_s.append(word2vec_model.similarity(f'{rdf}', f'{triple[1]}'))\n",
    "        if len(rel_s)<1 and len(rdf_l)>=1:#word2vecに存在する単語がなくて類似語が得られなかった場合は最初のを入れる\n",
    "            return rdf_l[0]\n",
    "    del word2vec_model\n",
    "    if len(rel_s) > 0:\n",
    "        sim_rel=rdf_l[rel_s.index(max(rel_s))]\n",
    "        print(f'類似述語:{sim_rel}')\n",
    "        return sim_rel\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#情報欠損検出ー＞述語修正失敗したときの処理と最後の要素引数以外おk\n",
    "def util(x, y, arr, target):\n",
    "    import re\n",
    "    try:\n",
    "        main_l=[]\n",
    "        sub_l=[]\n",
    "        if x in target:\n",
    "            main_l=[w for w in arr[target.index(x)] if w!='']\n",
    "            if y in target:\n",
    "                l_duplicate=[s for s in set(arr[target.index(x)]) if s!='']\n",
    "                for d in l_duplicate:\n",
    "                    sub=[]\n",
    "                    for i, a in enumerate(arr[target.index(x)]):\n",
    "                        if d==a:\n",
    "                            if arr[target.index(y)][i]!='':\n",
    "                                sub.append(arr[target.index(y)][i])\n",
    "                    if len(sub)>1:\n",
    "                        sub_l.append(sub)\n",
    "        main_l=[s for s in main_l if re.findall(r'_:b([0-9])+', s)==[]]+sub_l\n",
    "        return main_l\n",
    "    except IndexError:\n",
    "        return []\n",
    "\n",
    "def inspector(rdf_result, triple):\n",
    "    import pprint\n",
    "    import re\n",
    "    of=''\n",
    "    \n",
    "    num_of=0 #ofの検索結果数\n",
    "    rdf_list=rdf_result.split('\\n')\n",
    "    target=rdf_list.pop(0).split(',')#検索結果で得られた対象sro,of,b,bof\n",
    "    for i, rdf in enumerate(rdf_list):\n",
    "        if len(rdf)<=1:\n",
    "            rdf_list.pop(i)\n",
    "    print(rdf_list)\n",
    "    for i in range(len(rdf_list)):\n",
    "        rdf_list[i]=re.sub('http://localhost:3030/link/', '', rdf_list[i])\n",
    "        print(rdf_list[i])\n",
    "        rdf_list[i]=re.sub('http://localhost:3030/', '', rdf_list[i])\n",
    "        rdf_list[i]=re.sub(r'http://www.semanticweb.org/kojima/ontologies/v3/#', '', rdf_list[i])\n",
    "        rdf_list[i]=re.sub(r'http://www.w3.org/1999/02/22-rdf-syntax-ns#Bag', '', rdf_list[i])\n",
    "        rdf_list[i]=rdf_list[i].split(',')\n",
    "    keyword_arr=[list(x) for x in list(zip(*rdf_list))] #e.g.[s, o]=[[hoge, hoge], [hoge, hoge]]\n",
    "    print(target)\n",
    "    print(keyword_arr)\n",
    "    of_l=util('of', 'bof', keyword_arr, target)\n",
    "    num_of=len(of_l)\n",
    "    sro=['s','r','o']\n",
    "    check=[]\n",
    "    posi=position(triple)\n",
    "    if not keyword_arr ==[]:\n",
    "        if 0 in posi:\n",
    "            check=[x for x in keyword_arr[target.index(sro[position(triple).index(0)])] if x!='']\n",
    "    else:\n",
    "        pprint.pprint('述語修正')\n",
    "        if triple[1]!='':\n",
    "            rel=rel_modi(triple)\n",
    "            if rel!='':\n",
    "                triple[1]=rel\n",
    "                rdf_result=retrieval(make_sparql(_type, triple))\n",
    "                return inspector(rdf_result, triple)\n",
    "            else:#類似述語なし\n",
    "                return [triple[0], triple[1], triple[2], '']\n",
    "    print(f'check:{check}')\n",
    "    if len(check)>1 or num_of>1:\n",
    "        pprint.pprint('曖昧性排除')\n",
    "        add_info=ask_back(target, keyword_arr, triple)\n",
    "        print(f'add:{add_info}')\n",
    "        print('\\n')\n",
    "        for word in add_info:\n",
    "            if word[0]=='s':\n",
    "                triple[0]=word[1]\n",
    "            elif word[0]=='r':\n",
    "                triple[1]=word[1]\n",
    "            elif word[0]=='o':\n",
    "                triple[2]=word[1]\n",
    "            if word[0]=='of':\n",
    "                of=word[1]\n",
    "        print(triple)\n",
    "        posi=position(triple)\n",
    "        print(posi)\n",
    "        if 0 in posi:\n",
    "            print('xxxx')\n",
    "            rdf_result=retrieval(make_sparql(_type, triple))\n",
    "            print(rdf_result)\n",
    "            string=inspector(rdf_result, triple) #[s,r,o,of]\n",
    "            print(string)\n",
    "            return string\n",
    "#        for word in add_info:#word=['s', 'hoge']\n",
    "#            print(word)\n",
    "#            if type(word[1])==list:\n",
    "#                string='[a rdf:Bag; '\n",
    "#                for i, w in enumerate(word[1]):\n",
    "#                    if i<= len(word)-1:\n",
    "#                        string+='rdf:_'+ str(i) + 'local:' + w + ';'\n",
    "#                    else:\n",
    "#                        string+='rdf:_' + str(i) + 'local:' + w + ']'\n",
    "#                word[1]=string\n",
    "#            if word[0]=='s':\n",
    "#                triple[0]=word[1]\n",
    "#            elif word[0]=='r':\n",
    "#                triple[1]=word[1]\n",
    "#            elif word[0]=='o':\n",
    "#                triple[2]=word[1]\n",
    "#            if word[0]=='of':\n",
    "#                of=word[1]\n",
    "        \n",
    "        ##################ここでさいけんさくのときbagは崩れる#########\n",
    "        \n",
    "        print(triple)\n",
    "    \n",
    "    rdf_result=re.sub('http://localhost:3030(/|/link/)', '', rdf_result)\n",
    "    rdf_result=rdf_result.split('\\n')\n",
    "    for i, w in enumerate(rdf_result):\n",
    "        if len(w)==0:\n",
    "            rdf_result.pop(i)\n",
    "    rdf_result.pop(0)\n",
    "    posi=position(triple)\n",
    "    if 0 in posi:\n",
    "        n=posi.index(0)\n",
    "        if n==0:\n",
    "            s=rdf_result[0].split(',')[0]\n",
    "            r=triple[1]\n",
    "            o=triple[2]\n",
    "        elif n==1:\n",
    "            r=rdf_result[0].split(',')[0]\n",
    "            s=triple[0]\n",
    "            o=triple[2]\n",
    "        elif n==2:\n",
    "            o=rdf_result[0].split(',')[0]\n",
    "            s=triple[0]\n",
    "            r=triple[1]\n",
    "    else:\n",
    "        s=triple[0]\n",
    "        r=triple[1]\n",
    "        o=triple[2]\n",
    "    string=[s,r,o,of]\n",
    "    print(string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#応答選択　トリプル＋質問タイプ─＞応答文　#あとは言葉の検討\n",
    "def answer_select(rdfinfo, qtype):\n",
    "    import re\n",
    "    import pickle\n",
    "    from gensim import models\n",
    "    import numpy as np\n",
    "    import pprint\n",
    "    from pyknp import KNP\n",
    "    import warnings\n",
    "    def split_text(text):\n",
    "        from pyknp import Jumanpp\n",
    "        flag=0 #複合語検出用\n",
    "        jumanpp = Jumanpp()\n",
    "        result = jumanpp.analysis(text)\n",
    "        st_wakati=''\n",
    "        for mrph in result.mrph_list():\n",
    "            hinsi = mrph.hinsi\n",
    "            word = mrph.midasi\n",
    "            if flag == 0:\n",
    "                if st_wakati=='':\n",
    "                    st_wakati=word\n",
    "                else:\n",
    "                    st_wakati+=' '+ word\n",
    "            elif flag == 1 and hinsi=='名詞':#単語が名詞で前の単語も名詞\n",
    "                st_wakati+=word\n",
    "            if hinsi in [\"名詞\"] and ('時相' not in mrph.bunrui and '数' not in mrph.bunrui):\n",
    "                flag=1\n",
    "            else:\n",
    "                flag=0\n",
    "        return st_wakati\n",
    "    #why, how, when, where, what\n",
    "    ans_key=['からです', '方法です', '場面です', '時です', '']\n",
    "    string=''\n",
    "    for i,t in enumerate(rdfinfo):\n",
    "        if type(t)==list:\n",
    "            print(t)\n",
    "            rdfinfo[i]='と'.join(t)\n",
    "    if rdfinfo[3]!='':\n",
    "        string+=f'{rdfinfo[3]}の'\n",
    "    k=KNP(option='-tab -anaphora', jumanpp=True)\n",
    "    re=k.parse(rdfinfo[1]) #述語の品詞\n",
    "    bnst_l=re.bnst_list()\n",
    "    for bnst in bnst_l:\n",
    "        bnst_f=bnst.fstring\n",
    "        if rdfinfo[1]=='have':\n",
    "            string=f'{rdfinfo[0]}には{rdfinfo[2]}がある'\n",
    "        elif bnst_f.find('<用言:動>')>=0 or bnst_f.find('<用言:形>')>=0:\n",
    "            string+=f'{rdfinfo[0]}は{rdfinfo[2]}を{rdfinfo[1]}'\n",
    "        elif rdfinfo[1]=='is_a' or (bnst_f.find('<用言:判>')>=0 and bnst_f.find('サ変')<0):\n",
    "            string+=f'{rdfinfo[0]}は{rdfinfo[2]}である'\n",
    "        else:\n",
    "            string+=f'{rdfinfo[0]}は{rdfinfo[2]}を{rdfinfo[1]}する'   \n",
    "    string+=ans_key[qtype]\n",
    "    return string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "質問：パターンマイニングは何を発見しますか\n",
      "'input:パターンマイニングは何を発見しますか'\n",
      "['<項構造:発見:動:ガ/パターンマイニング;ヲ/何>']\n",
      "[[0, 0]]\n",
      "[['ガ', 'ヲ']]\n",
      "[['パターンマイニング', '何']]\n",
      "['発見']\n",
      "\"項構造リスト:[['パターンマイニング', '発見', '何']]\"\n",
      "\"質問内容項構造['パターンマイニング', '発見', '何']\"\n",
      "array([0.22471488, 0.20748518, 0.22411484, 0.24645595, 0.33707654])\n",
      "'normalization:[0.13295401 0.         0.12832376 0.30072042 1.        ]'\n",
      "array([0.48442608, 0.37997413, 0.56345916, 0.49945363, 1.        ])\n",
      "'normalization:[0.16846386 0.         0.29593125 0.19270084 1.        ]'\n",
      "'バイアス付与'\n",
      "'正規化類似度[0.15425992 0.         0.22888826 0.23590867 1.        ]'\n",
      "'質問タイプ:what'\n",
      "\n",
      "    PREFIX : <http://www.semanticweb.org/kojima/ontologies/v3/>\n",
      "    PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
      "    PREFIX link: <http://localhost:3030/link/>\n",
      "    PREFIX local: <http://localhost:3030/>\n",
      "    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
      "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
      "    select distinct ?o  ?b ?of ?bof \n",
      "    where {\n",
      "        {\n",
      "        \n",
      "                {\n",
      "                \n",
      "            {\n",
      "                :\\#パターンマイニング link:発見 ?o  .\n",
      "            }\n",
      "            union{\n",
      "                \n",
      "                ?b link:発見 ?o.\n",
      "                ?b ?x local:パターンマイニング.\n",
      "                \n",
      "                filter(isBlank(?b) && regex(str(?x), str(rdf:_)))\n",
      "            }\n",
      "            \n",
      "                union{\n",
      "                \n",
      "            {\n",
      "                :\\#パターンマイニング link:発見 ?o  .\n",
      "            }\n",
      "            union{\n",
      "                \n",
      "                ?b link:発見 ?o.\n",
      "                ?b ?x local:パターンマイニング.\n",
      "                \n",
      "                filter(isBlank(?b) && regex(str(?x), str(rdf:_)))\n",
      "            }\n",
      "            \n",
      "                optional{\n",
      "                ?o ?x ?b.\n",
      "                filter(isBlank(?o) && regex(str(?x), str(rdf:_)))\n",
      "                }\n",
      "                }\n",
      "                 \n",
      "                }\n",
      "                union\n",
      "                {\n",
      "                local:パターンマイニング link:of ?of .\n",
      "                optional{\n",
      "                ?of ?y ?bof.\n",
      "                filter(isBlank(?of))\n",
      "                }\n",
      "                }\n",
      "            \n",
      "        }\n",
      "    }\n",
      "    LIMIT 50\n",
      "    \n",
      "o,b,of,bof\n",
      "http://www.semanticweb.org/kojima/ontologies/v3/#問題,,,\n",
      "http://localhost:3030/規則,,,\n",
      "http://localhost:3030/手法,,,\n",
      "\n",
      "['http://www.semanticweb.org/kojima/ontologies/v3/#問題,,,', 'http://localhost:3030/規則,,,', 'http://localhost:3030/手法,,,']\n",
      "http://www.semanticweb.org/kojima/ontologies/v3/#問題,,,\n",
      "http://localhost:3030/規則,,,\n",
      "http://localhost:3030/手法,,,\n",
      "['o', 'b', 'of', 'bof']\n",
      "[['http://www.semanticweb.org/kojima/ontologies/v3/#問題', '規則', '手法'], ['', '', ''], ['', '', ''], ['', '', '']]\n",
      "check:['http://www.semanticweb.org/kojima/ontologies/v3/#問題', '規則', '手法']\n",
      "'曖昧性排除'\n",
      "yyyyy\n",
      "['o']\n",
      "in ask_back:['http://www.semanticweb.org/kojima/ontologies/v3/#問題', '規則', '手法']\n",
      "[1, 1, 0]\n",
      "パターンマイニングのhttp://www.semanticweb.org/kojima/ontologies/v3/#問題のことですか？(はい or いいえ)いいえ\n",
      "パターンマイニングの規則のことですか？(はい or いいえ)はい\n",
      "[['o', '規則']]\n",
      "add:[['o', '規則']]\n",
      "\n",
      "\n",
      "['パターンマイニング', '発見', '規則']\n",
      "[1, 1, 1]\n",
      "['パターンマイニング', '発見', '規則']\n",
      "['パターンマイニング', '発見', '規則', '']\n",
      "'output:パターンマイニングは規則を発見する'\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "from pyknp import KNP\n",
    "\n",
    "input_q=input('質問：')\n",
    "pprint.pprint(f'input:{input_q}')\n",
    "knp=KNP(option='-tab -anaphora', jumanpp=True)\n",
    "\n",
    "result=knp.parse(input_q)\n",
    "\n",
    "bnst_list=result.bnst_list()\n",
    "intr=''\n",
    "for bnst in bnst_list:\n",
    "    if '<疑問詞>' in bnst.fstring:\n",
    "        begin=bnst.fstring.find('正規化代表表記:')\n",
    "        end=bnst.fstring.find(r'/', begin+1)\n",
    "        intr=bnst.fstring[begin + len('正規化代表表記:') : end]\n",
    "term_l=term_extractor(result)\n",
    "print(term_l)\n",
    "term_l=term_inspection(term_l)\n",
    "if term_l==[]:\n",
    "    term=term_dependency(input_q)\n",
    "    term_l.append(term)\n",
    "tuple_l=tuple_maker(term_l, input_q)\n",
    "pprint.pprint(f'項構造リスト:{tuple_l}')\n",
    "_tuple=choose_tuple(tuple_l, result)\n",
    "pprint.pprint(f'質問内容項構造{_tuple}')\n",
    "_type=question_type(intr, input_q, _tuple)\n",
    "tmp=['why', 'how', 'where', 'when', 'what']\n",
    "pprint.pprint(f'質問タイプ:{tmp[_type]}')\n",
    "sparql=make_sparql(_type, _tuple)\n",
    "print(sparql)\n",
    "rdfinfo=retrieval(sparql)\n",
    "print(rdfinfo)\n",
    "r=inspector(rdfinfo, _tuple)\n",
    "s=answer_select(r, _type)\n",
    "pprint.pprint(f'output:{s}')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
