{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 形態素解析\n",
    "形態素解析とは、自然言語を形態素にまで分割することです。  \n",
    "形態素とは、言葉が意味を持つまとまりの単語の最小単位のことです。  \n",
    "今回は、形態素解析を用いて単語に分割します。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Janomeのインストール\n",
    "Janomeは日本語の形態素解析が可能なツールです。  \n",
    "Janomeは以下のコマンドでインストール可能です。  \n",
    "\n",
    "**pip install janome**\n",
    "\n",
    "Janomeを使って形態素解析を行いましょう。  \n",
    "Tokenizerをインポートします。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "t = Tokenizer()\n",
    "\n",
    "s = \"すもももももももものうち\"\n",
    "\n",
    "for token in t.tokenize(s):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分かち書き\n",
    "Janomeを使って分かち書きを行います。  \n",
    "分かち書きとは、文章を単語ごとに分割することです。  \n",
    "`tokenize`の際に引数を`wakati=True`にすることで、各単語をリストに格納できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "t = Tokenizer()\n",
    "\n",
    "s = \"すもももももももものうち\"\n",
    "\n",
    "word_list = t.tokenize(s, wakati=True)\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## コーパスを分かち書き\n",
    "前回前処理を行った「我輩は猫である」に対して、分かち書きを行います。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "import pickle\n",
    "\n",
    "t = Tokenizer()\n",
    "\n",
    "with open('wagahai_list.pickle', mode='rb') as f:\n",
    "    wagahai_list = pickle.load(f)\n",
    "\n",
    "for sentence in wagahai_list:\n",
    "    print(t.tokenize(sentence, wakati=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collectionsを使うことで、各単語の出現回数をカウントすることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "t = Tokenizer()\n",
    "\n",
    "words = []\n",
    "for sentence in wagahai_list:\n",
    "    words += t.tokenize(sentence, wakati=True)   # リストwordsに全ての単語を入れる\n",
    "\n",
    "c = collections.Counter(words)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題:\n",
    "前回の課題で前処理した「銀河鉄道の夜」で各単語数をカウントしてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
