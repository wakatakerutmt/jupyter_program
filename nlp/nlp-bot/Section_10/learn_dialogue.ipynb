{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 対話の学習\n",
    "宮沢賢治の小説において、次の文章を予測できるようにSeq2Seqのモデルを訓練します。  \n",
    "これにより、賢治風の返答が生成できるようになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用する文字\n",
    "学習をなるべく簡単にするために、ひらがなとカタカナ、記号のみを使用します。  \n",
    "コーパスで使われていない文字の入力にも対応するために、全てのひらがなとカタカナを用意します。  \n",
    "また、それ以外の記号などについては、コーパスで使われているものを加えます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t', '\\n', '、', '。', '々', 'ぁ', 'あ', 'ぃ', 'い', 'ぅ', 'う', 'ぇ', 'え', 'ぉ', 'お', 'か', 'が', 'き', 'ぎ', 'く', 'ぐ', 'け', 'げ', 'こ', 'ご', 'さ', 'ざ', 'し', 'じ', 'す', 'ず', 'せ', 'ぜ', 'そ', 'ぞ', 'た', 'だ', 'ち', 'ぢ', 'っ', 'つ', 'づ', 'て', 'で', 'と', 'ど', 'な', 'に', 'ぬ', 'ね', 'の', 'は', 'ば', 'ぱ', 'ひ', 'び', 'ぴ', 'ふ', 'ぶ', 'ぷ', 'へ', 'べ', 'ぺ', 'ほ', 'ぼ', 'ぽ', 'ま', 'み', 'む', 'め', 'も', 'ゃ', 'や', 'ゅ', 'ゆ', 'ょ', 'よ', 'ら', 'り', 'る', 'れ', 'ろ', 'ゎ', 'わ', 'ゐ', 'ゑ', 'を', 'ん', 'ァ', 'ア', 'ィ', 'イ', 'ゥ', 'ウ', 'ェ', 'エ', 'ォ', 'オ', 'カ', 'ガ', 'キ', 'ギ', 'ク', 'グ', 'ケ', 'ゲ', 'コ', 'ゴ', 'サ', 'ザ', 'シ', 'ジ', 'ス', 'ズ', 'セ', 'ゼ', 'ソ', 'ゾ', 'タ', 'ダ', 'チ', 'ヂ', 'ッ', 'ツ', 'ヅ', 'テ', 'デ', 'ト', 'ド', 'ナ', 'ニ', 'ヌ', 'ネ', 'ノ', 'ハ', 'バ', 'パ', 'ヒ', 'ビ', 'ピ', 'フ', 'ブ', 'プ', 'ヘ', 'ベ', 'ペ', 'ホ', 'ボ', 'ポ', 'マ', 'ミ', 'ム', 'メ', 'モ', 'ャ', 'ヤ', 'ュ', 'ユ', 'ョ', 'ヨ', 'ラ', 'リ', 'ル', 'レ', 'ロ', 'ヮ', 'ワ', 'ヰ', 'ヱ', 'ヲ', 'ン', 'ヴ', '・', 'ー', '？']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "hiragana = \"ぁあぃいぅうぇえぉおかがきぎくぐけげこごさざしじすずせぜそぞ\\\n",
    "ただちぢっつづてでとどなにぬねのはばぱひびぴふぶぷへべぺほぼぽ\\\n",
    "まみむめもゃやゅゆょよらりるれろゎわゐゑをん\"\n",
    "\n",
    "katakana = \"ァアィイゥウェエォオカガキギクグケゲコゴサザシジスズセゼソゾ\\\n",
    "タダチヂッツヅテデトドナニヌネノハバパヒビピフブプヘベペホボポ\\\n",
    "マミムメモャヤュユョヨラリルレロヮワヰヱヲンヴ\"\n",
    "\n",
    "chars = hiragana + katakana\n",
    "\n",
    "with open(\"kana_kenji.txt\", mode=\"r\", encoding=\"utf-8\") as f:  # 前回保存したファイル\n",
    "    text = f.read()\n",
    "    \n",
    "for char in text:  # ひらがな、カタカナ以外でコーパスに使われている文字を追加\n",
    "    if char not in chars:\n",
    "        chars += char\n",
    "        \n",
    "chars += \"\\t\\n\"  # タブと改行を追加\n",
    "        \n",
    "chars_list = sorted(list(chars))  # 文字列をリストに変換してソートする\n",
    "print(chars_list)\n",
    "\n",
    "with open(\"kana_chars.pickle\", mode=\"wb\") as f:  # pickleで保存\n",
    "    pickle.dump(chars_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文字のベクトル化\n",
    "各文字をone-hot表現で表し、encoderへの入力、decoderへの入力、decoderの正解を作成します。  \n",
    "各文章はそれぞれ長さが違いますが、文章の終了後は全て0のベクトルで埋めます。  \n",
    "学習効率を考慮し、長すぎる文章はカットします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5091, 128, 175)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# インデックスと文字で辞書を作成\n",
    "char_indices = {}  # 文字がキーでインデックスが値\n",
    "for i, char in enumerate(chars_list):\n",
    "    char_indices[char] = i\n",
    "indices_char = {}  # インデックスがキーで文字が値\n",
    "for i, char in enumerate(chars_list):\n",
    "    indices_char[i] = char\n",
    "    \n",
    "seperator = \"。\"\n",
    "sentence_list = text.split(seperator) \n",
    "sentence_list.pop() \n",
    "sentence_list = [x+seperator for x in sentence_list]\n",
    "\n",
    "max_sentence_length = 128  # 文章の最大長さ。これより長い文章はカットされる。\n",
    "sentence_list = [sentence for sentence in sentence_list if len(sentence) <= max_sentence_length]  # 長すぎる文章のカット\n",
    "\n",
    "n_char = len(chars_list)  # 文字の種類の数\n",
    "n_sample = len(sentence_list) - 1  # サンプル数\n",
    "\n",
    "x_sentences = []  # 入力の文章\n",
    "t_sentences = []  # 正解の文章\n",
    "for i in range(n_sample):\n",
    "    x_sentences.append(sentence_list[i])\n",
    "    t_sentences.append(\"\\t\" + sentence_list[i+1] + \"\\n\")  # 正解は先頭にタブ、末尾に改行を加える\n",
    "max_length_x = max_sentence_length  # 入力文章の最大長さ\n",
    "max_length_t = max_sentence_length + 2  # 正解文章の最大長さ\n",
    "\n",
    "x_encoder = np.zeros((n_sample, max_length_x, n_char), dtype=np.bool)  # encoderへの入力\n",
    "x_decoder = np.zeros((n_sample, max_length_t, n_char), dtype=np.bool)  # decoderへの入力\n",
    "t_decoder = np.zeros((n_sample, max_length_t, n_char), dtype=np.bool)  # decoderの正解\n",
    "\n",
    "for i in range(n_sample):\n",
    "    x_sentence = x_sentences[i]\n",
    "    t_sentence = t_sentences[i]\n",
    "    for j, char in enumerate(x_sentence):\n",
    "        x_encoder[i, j, char_indices[char]] = 1  # encoderへの入力をone-hot表現で表す\n",
    "    for j, char in enumerate(t_sentence):\n",
    "        x_decoder[i, j, char_indices[char]] = 1  # decoderへの入力をone-hot表現で表す\n",
    "        if j > 0:  # 正解は入力より1つ前の時刻のものにする\n",
    "            t_decoder[i, j-1, char_indices[char]] = 1\n",
    "            \n",
    "print(x_encoder.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 各設定\n",
    "学習に関する各設定です。  \n",
    "「早期終了」により学習を自動ストップするので、エポック数は多めに設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 1000\n",
    "n_mid = 256  # 中間層のニューロン数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習用モデルの構築\n",
    "学習用のSeq2Seqモデルを構築します。  \n",
    "今回は、前のセクションでより自然な文章の作成につながったGRUを使います。  \n",
    "また、入力の直後にMasking層を挟みます。\n",
    "これにより、全ての要素が0であるベクトルの入力は無視されます。  \n",
    "\n",
    "GRU層にはdropoutを設定し、ニューロンをランダムに無効にすることで過学習対策をします。  \n",
    "過学習とは、モデルが訓練データに過剰に適応してしまい、未知のデータに対して機能しなくなってしまうことです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 175)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None, 175)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, None, 175)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "masking_2 (Masking)             (None, None, 175)    0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     [(None, 256), (None, 331776      masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gru_2 (GRU)                     [(None, None, 256),  331776      masking_2[0][0]                  \n",
      "                                                                 gru_1[0][1]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 175)    44975       gru_2[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 708,527\n",
      "Trainable params: 708,527\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, GRU, Input, Masking\n",
    "\n",
    "encoder_input = Input(shape=(None, n_char))\n",
    "encoder_mask = Masking(mask_value=0)  # 全ての要素が0であるベクトルの入力は無視する\n",
    "encoder_masked = encoder_mask(encoder_input)\n",
    "encoder_lstm = GRU(n_mid, dropout=0.2, recurrent_dropout=0.2, return_state=True)  # dropoutを設定し、ニューロンをランダムに無効にする\n",
    "encoder_output, encoder_state_h = encoder_lstm(encoder_masked)\n",
    "\n",
    "decoder_input = Input(shape=(None, n_char))\n",
    "decoder_mask = Masking(mask_value=0)  # 全ての要素が0であるベクトルの入力は無視する\n",
    "decoder_masked = decoder_mask(decoder_input)\n",
    "decoder_lstm = GRU(n_mid, dropout=0.2, recurrent_dropout=0.2, return_sequences=True, return_state=True)  # dropoutを設定\n",
    "decoder_output, _ = decoder_lstm(decoder_masked, initial_state=encoder_state_h)  # encoderの状態を初期状態にする\n",
    "decoder_dense = Dense(n_char, activation='softmax')\n",
    "decoder_output = decoder_dense(decoder_output)\n",
    "\n",
    "model = Model([encoder_input, decoder_input], decoder_output)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n",
    "構築したSeq2Seqのモデルを使って、学習を行います。  \n",
    "今回は、**早期終了**を設定します。  \n",
    "コールバックにEarlyStoppingを設定することで、学習を自動で終了させることができます。  \n",
    "誤差に改善が見られなくなってからpatianceで設定したエポック数が経過すると、学習は終了となります。  \n",
    "お手元の環境にもよりますが学習には数時間程度必要ですので、早く結果を確認したい方はpatienceの値を小さくしましょう。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4581 samples, validate on 510 samples\n",
      "Epoch 1/1000\n",
      "4581/4581 [==============================] - 138s 30ms/step - loss: 3.7477 - val_loss: 3.4353\n",
      "Epoch 2/1000\n",
      "4581/4581 [==============================] - 138s 30ms/step - loss: 3.3154 - val_loss: 3.2225\n",
      "Epoch 3/1000\n",
      "4581/4581 [==============================] - 142s 31ms/step - loss: 3.1636 - val_loss: 3.1211\n",
      "Epoch 4/1000\n",
      "4581/4581 [==============================] - 145s 32ms/step - loss: 3.0768 - val_loss: 3.0734\n",
      "Epoch 5/1000\n",
      "4581/4581 [==============================] - 141s 31ms/step - loss: 3.0233 - val_loss: 3.0197\n",
      "Epoch 6/1000\n",
      "4581/4581 [==============================] - 142s 31ms/step - loss: 2.9817 - val_loss: 2.9727\n",
      "Epoch 7/1000\n",
      "4581/4581 [==============================] - 137s 30ms/step - loss: 2.9453 - val_loss: 2.9475\n",
      "Epoch 8/1000\n",
      "4581/4581 [==============================] - 135s 30ms/step - loss: 2.9179 - val_loss: 2.9204\n",
      "Epoch 9/1000\n",
      "4581/4581 [==============================] - 135s 29ms/step - loss: 2.8843 - val_loss: 2.8974\n",
      "Epoch 10/1000\n",
      "4581/4581 [==============================] - 135s 29ms/step - loss: 2.8568 - val_loss: 2.8747\n",
      "Epoch 11/1000\n",
      "4581/4581 [==============================] - 134s 29ms/step - loss: 2.8353 - val_loss: 2.8608\n",
      "Epoch 12/1000\n",
      "4581/4581 [==============================] - 134s 29ms/step - loss: 2.8074 - val_loss: 2.8305\n",
      "Epoch 13/1000\n",
      "4581/4581 [==============================] - 135s 29ms/step - loss: 2.7854 - val_loss: 2.8117\n",
      "Epoch 14/1000\n",
      "4581/4581 [==============================] - 135s 29ms/step - loss: 2.7649 - val_loss: 2.7928\n",
      "Epoch 15/1000\n",
      "4581/4581 [==============================] - 135s 29ms/step - loss: 2.7407 - val_loss: 2.7759\n",
      "Epoch 16/1000\n",
      "4581/4581 [==============================] - 134s 29ms/step - loss: 2.7267 - val_loss: 2.7640\n",
      "Epoch 17/1000\n",
      "4581/4581 [==============================] - 134s 29ms/step - loss: 2.7038 - val_loss: 2.7468\n",
      "Epoch 18/1000\n",
      "4581/4581 [==============================] - 135s 29ms/step - loss: 2.6854 - val_loss: 2.7324\n",
      "Epoch 19/1000\n",
      "4581/4581 [==============================] - 135s 29ms/step - loss: 2.6717 - val_loss: 2.7207\n",
      "Epoch 20/1000\n",
      "4581/4581 [==============================] - 135s 29ms/step - loss: 2.6550 - val_loss: 2.7165\n",
      "Epoch 21/1000\n",
      "4581/4581 [==============================] - 137s 30ms/step - loss: 2.6405 - val_loss: 2.7062\n",
      "Epoch 22/1000\n",
      "4581/4581 [==============================] - 136s 30ms/step - loss: 2.6252 - val_loss: 2.6887\n",
      "Epoch 23/1000\n",
      "4581/4581 [==============================] - 141s 31ms/step - loss: 2.6079 - val_loss: 2.6874\n",
      "Epoch 24/1000\n",
      "4581/4581 [==============================] - 142s 31ms/step - loss: 2.5952 - val_loss: 2.6770\n",
      "Epoch 25/1000\n",
      "4581/4581 [==============================] - 138s 30ms/step - loss: 2.5809 - val_loss: 2.6619\n",
      "Epoch 26/1000\n",
      "4581/4581 [==============================] - 140s 30ms/step - loss: 2.5664 - val_loss: 2.6630\n",
      "Epoch 27/1000\n",
      "4581/4581 [==============================] - 137s 30ms/step - loss: 2.5548 - val_loss: 2.6525\n",
      "Epoch 28/1000\n",
      "4581/4581 [==============================] - 144s 31ms/step - loss: 2.5505 - val_loss: 2.6519\n",
      "Epoch 29/1000\n",
      "4581/4581 [==============================] - 143s 31ms/step - loss: 2.5392 - val_loss: 2.6394\n",
      "Epoch 30/1000\n",
      "4581/4581 [==============================] - 139s 30ms/step - loss: 2.5203 - val_loss: 2.6322\n",
      "Epoch 31/1000\n",
      "4581/4581 [==============================] - 156s 34ms/step - loss: 2.5178 - val_loss: 2.6220\n",
      "Epoch 32/1000\n",
      "4581/4581 [==============================] - 151s 33ms/step - loss: 2.5027 - val_loss: 2.6263\n",
      "Epoch 33/1000\n",
      "4581/4581 [==============================] - 135s 30ms/step - loss: 2.4974 - val_loss: 2.6158\n",
      "Epoch 34/1000\n",
      "4581/4581 [==============================] - 134s 29ms/step - loss: 2.4831 - val_loss: 2.6149\n",
      "Epoch 35/1000\n",
      "4581/4581 [==============================] - 135s 30ms/step - loss: 2.4767 - val_loss: 2.6095\n",
      "Epoch 36/1000\n",
      "4581/4581 [==============================] - 135s 29ms/step - loss: 2.4703 - val_loss: 2.6047\n",
      "Epoch 37/1000\n",
      "4581/4581 [==============================] - 134s 29ms/step - loss: 2.4563 - val_loss: 2.5974\n",
      "Epoch 38/1000\n",
      "4581/4581 [==============================] - 134s 29ms/step - loss: 2.4493 - val_loss: 2.5945\n",
      "Epoch 39/1000\n",
      "4581/4581 [==============================] - 133s 29ms/step - loss: 2.4419 - val_loss: 2.5833\n",
      "Epoch 40/1000\n",
      "4581/4581 [==============================] - 134s 29ms/step - loss: 2.4342 - val_loss: 2.5842\n",
      "Epoch 41/1000\n",
      "4581/4581 [==============================] - 134s 29ms/step - loss: 2.4230 - val_loss: 2.5864\n",
      "Epoch 42/1000\n",
      "4581/4581 [==============================] - 133s 29ms/step - loss: 2.4212 - val_loss: 2.5892\n",
      "Epoch 43/1000\n",
      "4581/4581 [==============================] - 133s 29ms/step - loss: 2.4102 - val_loss: 2.5815\n",
      "Epoch 44/1000\n",
      "4581/4581 [==============================] - 133s 29ms/step - loss: 2.4083 - val_loss: 2.5772\n",
      "Epoch 45/1000\n",
      "4581/4581 [==============================] - 134s 29ms/step - loss: 2.4017 - val_loss: 2.5701\n",
      "Epoch 46/1000\n",
      "4581/4581 [==============================] - 133s 29ms/step - loss: 2.3911 - val_loss: 2.5677\n",
      "Epoch 47/1000\n",
      "4581/4581 [==============================] - 133s 29ms/step - loss: 2.3829 - val_loss: 2.5575\n",
      "Epoch 48/1000\n",
      "4581/4581 [==============================] - 134s 29ms/step - loss: 2.3777 - val_loss: 2.5675\n",
      "Epoch 49/1000\n",
      "4581/4581 [==============================] - 136s 30ms/step - loss: 2.3714 - val_loss: 2.5595\n",
      "Epoch 50/1000\n",
      "4581/4581 [==============================] - 143s 31ms/step - loss: 2.3704 - val_loss: 2.5634\n",
      "Epoch 51/1000\n",
      "4581/4581 [==============================] - 143s 31ms/step - loss: 2.3614 - val_loss: 2.5527\n",
      "Epoch 52/1000\n",
      "4581/4581 [==============================] - 136s 30ms/step - loss: 2.3540 - val_loss: 2.5581\n",
      "Epoch 53/1000\n",
      "4581/4581 [==============================] - 138s 30ms/step - loss: 2.3483 - val_loss: 2.5570\n",
      "Epoch 54/1000\n",
      "4581/4581 [==============================] - 143s 31ms/step - loss: 2.3449 - val_loss: 2.5586\n",
      "Epoch 55/1000\n",
      "4581/4581 [==============================] - 146s 32ms/step - loss: 2.3355 - val_loss: 2.5523\n",
      "Epoch 56/1000\n",
      "4581/4581 [==============================] - 137s 30ms/step - loss: 2.3362 - val_loss: 2.5501\n",
      "Epoch 57/1000\n",
      "4581/4581 [==============================] - 139s 30ms/step - loss: 2.3299 - val_loss: 2.5563\n",
      "Epoch 58/1000\n",
      "4581/4581 [==============================] - 140s 31ms/step - loss: 2.3248 - val_loss: 2.5517\n",
      "Epoch 59/1000\n",
      "4581/4581 [==============================] - 140s 30ms/step - loss: 2.3179 - val_loss: 2.5549\n",
      "Epoch 60/1000\n",
      "4581/4581 [==============================] - 135s 29ms/step - loss: 2.3180 - val_loss: 2.5495\n",
      "Epoch 61/1000\n",
      "4581/4581 [==============================] - 135s 29ms/step - loss: 2.3122 - val_loss: 2.5449\n",
      "Epoch 62/1000\n",
      "4581/4581 [==============================] - 139s 30ms/step - loss: 2.3060 - val_loss: 2.5415\n",
      "Epoch 63/1000\n",
      "4581/4581 [==============================] - 139s 30ms/step - loss: 2.3062 - val_loss: 2.5467\n",
      "Epoch 64/1000\n",
      "4581/4581 [==============================] - 136s 30ms/step - loss: 2.2996 - val_loss: 2.5457\n",
      "Epoch 65/1000\n",
      "4581/4581 [==============================] - 135s 30ms/step - loss: 2.2948 - val_loss: 2.5409\n",
      "Epoch 66/1000\n",
      "4581/4581 [==============================] - 142s 31ms/step - loss: 2.2918 - val_loss: 2.5339\n",
      "Epoch 67/1000\n",
      "4581/4581 [==============================] - 140s 31ms/step - loss: 2.2851 - val_loss: 2.5417\n",
      "Epoch 68/1000\n",
      "4581/4581 [==============================] - 139s 30ms/step - loss: 2.2816 - val_loss: 2.5423\n",
      "Epoch 69/1000\n",
      "4581/4581 [==============================] - 135s 30ms/step - loss: 2.2827 - val_loss: 2.5377\n",
      "Epoch 70/1000\n",
      "4581/4581 [==============================] - 135s 29ms/step - loss: 2.2768 - val_loss: 2.5348\n",
      "Epoch 71/1000\n",
      "4581/4581 [==============================] - 135s 29ms/step - loss: 2.2720 - val_loss: 2.5387\n",
      "Epoch 72/1000\n",
      "4581/4581 [==============================] - 135s 29ms/step - loss: 2.2687 - val_loss: 2.5324\n",
      "Epoch 73/1000\n",
      "4581/4581 [==============================] - 135s 29ms/step - loss: 2.2643 - val_loss: 2.5387\n",
      "Epoch 74/1000\n",
      "4581/4581 [==============================] - 135s 29ms/step - loss: 2.2626 - val_loss: 2.5300\n",
      "Epoch 75/1000\n",
      "4581/4581 [==============================] - 135s 29ms/step - loss: 2.2595 - val_loss: 2.5322\n",
      "Epoch 76/1000\n",
      "4581/4581 [==============================] - 135s 29ms/step - loss: 2.2540 - val_loss: 2.5361\n",
      "Epoch 77/1000\n",
      "4581/4581 [==============================] - 137s 30ms/step - loss: 2.2527 - val_loss: 2.5341\n",
      "Epoch 78/1000\n",
      "4581/4581 [==============================] - 135s 30ms/step - loss: 2.2437 - val_loss: 2.5359\n",
      "Epoch 79/1000\n",
      "4581/4581 [==============================] - 135s 30ms/step - loss: 2.2490 - val_loss: 2.5281\n",
      "Epoch 80/1000\n",
      "4581/4581 [==============================] - 135s 30ms/step - loss: 2.2470 - val_loss: 2.5308\n",
      "Epoch 81/1000\n",
      "4581/4581 [==============================] - 137s 30ms/step - loss: 2.2412 - val_loss: 2.5292\n",
      "Epoch 82/1000\n",
      "4581/4581 [==============================] - 136s 30ms/step - loss: 2.2318 - val_loss: 2.5369\n",
      "Epoch 83/1000\n",
      "4581/4581 [==============================] - 137s 30ms/step - loss: 2.2325 - val_loss: 2.5378\n",
      "Epoch 84/1000\n",
      "4581/4581 [==============================] - 136s 30ms/step - loss: 2.2283 - val_loss: 2.5381\n",
      "Epoch 85/1000\n",
      "4581/4581 [==============================] - 135s 29ms/step - loss: 2.2232 - val_loss: 2.5320\n",
      "Epoch 86/1000\n",
      "4581/4581 [==============================] - 135s 29ms/step - loss: 2.2222 - val_loss: 2.5350\n",
      "Epoch 87/1000\n",
      "4581/4581 [==============================] - 137s 30ms/step - loss: 2.2185 - val_loss: 2.5390\n",
      "Epoch 88/1000\n",
      "4581/4581 [==============================] - 143s 31ms/step - loss: 2.2180 - val_loss: 2.5318\n",
      "Epoch 89/1000\n",
      "4581/4581 [==============================] - 138s 30ms/step - loss: 2.2126 - val_loss: 2.5346\n",
      "Epoch 90/1000\n",
      "4581/4581 [==============================] - 144s 31ms/step - loss: 2.2142 - val_loss: 2.5298\n",
      "Epoch 91/1000\n",
      "4581/4581 [==============================] - 143s 31ms/step - loss: 2.2077 - val_loss: 2.5311\n",
      "Epoch 92/1000\n",
      "4581/4581 [==============================] - 141s 31ms/step - loss: 2.2049 - val_loss: 2.5264\n",
      "Epoch 93/1000\n",
      "4581/4581 [==============================] - 142s 31ms/step - loss: 2.2060 - val_loss: 2.5269\n",
      "Epoch 94/1000\n",
      "4581/4581 [==============================] - 141s 31ms/step - loss: 2.1998 - val_loss: 2.5327\n",
      "Epoch 95/1000\n",
      "4581/4581 [==============================] - 144s 32ms/step - loss: 2.2028 - val_loss: 2.5278\n",
      "Epoch 96/1000\n",
      "4581/4581 [==============================] - 143s 31ms/step - loss: 2.1976 - val_loss: 2.5281\n",
      "Epoch 97/1000\n",
      "4581/4581 [==============================] - 139s 30ms/step - loss: 2.1926 - val_loss: 2.5375\n",
      "Epoch 98/1000\n",
      "4581/4581 [==============================] - 136s 30ms/step - loss: 2.1953 - val_loss: 2.5321\n",
      "Epoch 99/1000\n",
      "4581/4581 [==============================] - 136s 30ms/step - loss: 2.1919 - val_loss: 2.5258\n",
      "Epoch 100/1000\n",
      "4581/4581 [==============================] - 136s 30ms/step - loss: 2.1892 - val_loss: 2.5295\n",
      "Epoch 101/1000\n",
      "4581/4581 [==============================] - 135s 30ms/step - loss: 2.1848 - val_loss: 2.5297\n",
      "Epoch 102/1000\n",
      "4581/4581 [==============================] - 136s 30ms/step - loss: 2.1846 - val_loss: 2.5265\n",
      "Epoch 103/1000\n",
      "4581/4581 [==============================] - 135s 30ms/step - loss: 2.1834 - val_loss: 2.5279\n",
      "Epoch 104/1000\n",
      "4581/4581 [==============================] - 136s 30ms/step - loss: 2.1800 - val_loss: 2.5242\n",
      "Epoch 105/1000\n",
      "4581/4581 [==============================] - 136s 30ms/step - loss: 2.1775 - val_loss: 2.5352\n",
      "Epoch 106/1000\n",
      "4581/4581 [==============================] - 136s 30ms/step - loss: 2.1748 - val_loss: 2.5288\n",
      "Epoch 107/1000\n",
      "4581/4581 [==============================] - 136s 30ms/step - loss: 2.1689 - val_loss: 2.5340\n",
      "Epoch 108/1000\n",
      "4581/4581 [==============================] - 136s 30ms/step - loss: 2.1722 - val_loss: 2.5265\n",
      "Epoch 109/1000\n",
      "4581/4581 [==============================] - 141s 31ms/step - loss: 2.1666 - val_loss: 2.5299\n",
      "Epoch 110/1000\n",
      "4581/4581 [==============================] - 141s 31ms/step - loss: 2.1629 - val_loss: 2.5385\n",
      "Epoch 111/1000\n",
      "4581/4581 [==============================] - 143s 31ms/step - loss: 2.1651 - val_loss: 2.5305\n",
      "Epoch 112/1000\n",
      "4581/4581 [==============================] - 142s 31ms/step - loss: 2.1621 - val_loss: 2.5301\n",
      "Epoch 113/1000\n",
      "4581/4581 [==============================] - 138s 30ms/step - loss: 2.1564 - val_loss: 2.5356\n",
      "Epoch 114/1000\n",
      "4581/4581 [==============================] - 136s 30ms/step - loss: 2.1595 - val_loss: 2.5382\n",
      "Epoch 115/1000\n",
      "4581/4581 [==============================] - 136s 30ms/step - loss: 2.1536 - val_loss: 2.5356\n",
      "Epoch 116/1000\n",
      "4581/4581 [==============================] - 136s 30ms/step - loss: 2.1547 - val_loss: 2.5357\n",
      "Epoch 117/1000\n",
      "4581/4581 [==============================] - 135s 30ms/step - loss: 2.1527 - val_loss: 2.5383\n",
      "Epoch 118/1000\n",
      "4581/4581 [==============================] - 135s 30ms/step - loss: 2.1537 - val_loss: 2.5360\n",
      "Epoch 119/1000\n",
      "4581/4581 [==============================] - 135s 30ms/step - loss: 2.1492 - val_loss: 2.5357\n",
      "Epoch 120/1000\n",
      "4581/4581 [==============================] - 136s 30ms/step - loss: 2.1473 - val_loss: 2.5332\n",
      "Epoch 121/1000\n",
      "4581/4581 [==============================] - 136s 30ms/step - loss: 2.1494 - val_loss: 2.5388\n",
      "Epoch 122/1000\n",
      "4581/4581 [==============================] - 136s 30ms/step - loss: 2.1419 - val_loss: 2.5330\n",
      "Epoch 123/1000\n",
      "4581/4581 [==============================] - 137s 30ms/step - loss: 2.1467 - val_loss: 2.5385\n",
      "Epoch 124/1000\n",
      "4581/4581 [==============================] - 144s 32ms/step - loss: 2.1412 - val_loss: 2.5338\n",
      "Epoch 125/1000\n",
      "4581/4581 [==============================] - 149s 33ms/step - loss: 2.1358 - val_loss: 2.5307\n",
      "Epoch 126/1000\n",
      "4581/4581 [==============================] - 149s 32ms/step - loss: 2.1389 - val_loss: 2.5366\n",
      "Epoch 127/1000\n",
      "4581/4581 [==============================] - 142s 31ms/step - loss: 2.1325 - val_loss: 2.5387\n",
      "Epoch 128/1000\n",
      "4581/4581 [==============================] - 143s 31ms/step - loss: 2.1326 - val_loss: 2.5323\n",
      "Epoch 129/1000\n",
      "4581/4581 [==============================] - 149s 33ms/step - loss: 2.1288 - val_loss: 2.5417\n",
      "Epoch 130/1000\n",
      "4581/4581 [==============================] - 140s 31ms/step - loss: 2.1255 - val_loss: 2.5473\n",
      "Epoch 131/1000\n",
      "4581/4581 [==============================] - 139s 30ms/step - loss: 2.1240 - val_loss: 2.5360\n",
      "Epoch 132/1000\n",
      "4581/4581 [==============================] - 139s 30ms/step - loss: 2.1288 - val_loss: 2.5391\n",
      "Epoch 133/1000\n",
      "4581/4581 [==============================] - 138s 30ms/step - loss: 2.1275 - val_loss: 2.5386\n",
      "Epoch 134/1000\n",
      "4581/4581 [==============================] - 248s 54ms/step - loss: 2.1277 - val_loss: 2.5348\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping \n",
    "\n",
    "# val_lossに改善が見られなくなってから、30エポックで学習は終了\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=30) \n",
    "\n",
    "history = model.fit([x_encoder, x_decoder], t_decoder,\n",
    "                     batch_size=batch_size,\n",
    "                     epochs=epochs,\n",
    "                     validation_split=0.1,  # 10%は検証用\n",
    "                     callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習の推移\n",
    "誤差の推移を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.plot(np.arange(len(loss)), loss)\n",
    "plt.plot(np.arange(len(val_loss)), val_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "検証用データの誤差は途中で改善が止まっていますね。  \n",
    "もう少し早めに学習を終了しても良さそうです。  \n",
    "誤差の値自体はあまり小さくなっていませんが、とりあえず対話文の生成を試してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 予測用モデルの構築\n",
    "学習済みのオブジェクトから、encoder、decoderのモデルを個別に構築します。    \n",
    "encoderは入力を受け取って状態を返し、decoderは入力と状態を受け取って出力と状態を返すようにします。  \n",
    "構築したモデルは、後のレクチャーで使えるように保存しておきます。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/nlp_bot/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer gru_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "# encoderのモデル\n",
    "encoder_model = Model(encoder_input, encoder_state_h)\n",
    "\n",
    "# decoderのモデル\n",
    "decoder_state_in_h = Input(shape=(n_mid,))\n",
    "decoder_state_in = [decoder_state_in_h]\n",
    "\n",
    "decoder_output, decoder_state_h = decoder_lstm(decoder_input,\n",
    "                                               initial_state=decoder_state_in_h)\n",
    "decoder_output = decoder_dense(decoder_output)\n",
    "\n",
    "decoder_model = Model([decoder_input] + decoder_state_in,\n",
    "                      [decoder_output, decoder_state_h])\n",
    "\n",
    "# モデルの保存\n",
    "encoder_model.save('encoder_model.h5')\n",
    "decoder_model.save('decoder_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 返答作成用の関数\n",
    "入力を出力に変換し、返答を作成するための関数を設定します。  \n",
    "decoderでは、各時刻ごとに予測を行い、出力と状態を次の時刻に渡します。  \n",
    "decoderの出力を確率として捉え、その確率に従ってサンプリングを行うので実行するたびにやや異なる文章が生成されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond(input_data, beta=5):\n",
    "    state_value = encoder_model.predict(input_data)\n",
    "    y_decoder = np.zeros((1, 1, n_char))  # decoderの出力を格納する配列\n",
    "    y_decoder[0][0][char_indices[\"\\t\"]] = 1  # decoderの最初の入力はタブ。one-hot表現にする。\n",
    "\n",
    "    respond_sentence = \"\"  # 返答の文字列\n",
    "    while True:\n",
    "        y, h = decoder_model.predict([y_decoder, state_value])\n",
    "        p_power = y[0][0] ** beta  # 確率分布の調整\n",
    "        next_index = np.random.choice(len(p_power), p=p_power/np.sum(p_power)) \n",
    "        next_char = indices_char[next_index]  # 次の文字\n",
    "\n",
    "        if (next_char == \"\\n\" or len(respond_sentence) >= max_length_x):\n",
    "            break  # 次の文字が改行のとき、もしくは最大文字数を超えたときは終了\n",
    "            \n",
    "        respond_sentence += next_char\n",
    "        y_decoder = np.zeros((1, 1, n_char))  # 次の時刻の入力\n",
    "        y_decoder[0][0][next_index] = 1\n",
    "\n",
    "        state_value = h  # 次の時刻の状態\n",
    "\n",
    "    return respond_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 動作の確認\n",
    "訓練データの最初の100文を使って、どのような返答が返ってくるかを確かめます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ではみなさんは、そういうふうにかわだといわれたり、ちちのながれたあとだといわれたりしていたこのぼんやりとしろいものがほんとうはなにかごしょうちですか。\n",
      "Response: せんせいはまたいっっていました。\n",
      "\n",
      "Input: せんせいは、こくばんにつるしたおおきなくろいせいざのずの、うえからしたへしろくけぶったぎんがおびのようなところをさしながら、みんなにとをかけました。\n",
      "Response: おとうさんがもうしました。\n",
      "\n",
      "Input: カムパネルラがてをあげました。\n",
      "Response: そしてそのまえにはいっているのでした。\n",
      "\n",
      "Input: それからしごにんてをあげました。\n",
      "Response: それから、そのとき、あかいはかせは、あおいひかりをつかまえていましたが、そのときは、あかいはかせは、あかいはしらないというように、そのときあたまをかけて、いっしょうけんいのちになっていました。\n",
      "\n",
      "Input: ジョバンニもてをあげようとして、いそいでそのままやめました。\n",
      "Response: そしてそのおとこは、きっとみんなはなにかかおをあけて、いっしょに、いっしょに、いっしょうけんいのち、あかいひかりをして、それから、そのときあたまをかけて、それから、きっとまっていました。\n",
      "\n",
      "Input: たしかにあれがみんなほしだと、いつかざっしでよんだのでしたが、このごろはジョバンニはまるでまいにちきょうしつでもねむく、ほんをよむひまもよむほんもないので、なんだかどんなこともよくわからないというきもちがするのでした。\n",
      "Response: ところがそのこどもは、そのとき、あかいはいってきたのです。\n",
      "\n",
      "Input: ところがせんせいははやくもそれをみつけけたのでした。\n",
      "Response: そしてそのまえにはしっていました。\n",
      "\n",
      "Input: ジョバンニさん。\n",
      "Response: あなたはこんどはおおきなこえでした。\n",
      "\n",
      "Input: あなたはわかっているのでしょう。\n",
      "Response: ジョバンニは、なにかおもいました。\n",
      "\n",
      "Input: ジョバンニはせいよくたちあがりましたが、たってみるともうはっきりとそれをこたえることができないのでした。\n",
      "Response: そしてそのときは、あおいひかりをして、そのときあたまをあけて、いっしょうけんいのち、あかいのはらのほうへはいっていました。\n",
      "\n",
      "Input: ザネリがまえのせきからふりかえって、ジョバンニをみてくすっとわらいました。\n",
      "Response: ジョバンニは、もうそのときあるいていきました。\n",
      "\n",
      "Input: ジョバンニはもうどぎまぎしてまっあかになってしまいました。\n",
      "Response: そしてそのまえにはしっているのでした。\n",
      "\n",
      "Input: せんせいがまたいいました。\n",
      "Response: おれがいいました。\n",
      "\n",
      "Input: おおきなぼうえんきょうでぎんがをよっくしらべるとぎんがはだいたいなんでしょう。\n",
      "Response: アルコールがあるいているんだ。\n",
      "\n",
      "Input: やっぱりほしだとジョバンニはおもいましたがこんどもすぐにこたえることができませんでした。\n",
      "Response: ところがそのかわりは、あかいはいってきたのです。\n",
      "\n",
      "Input: せんせいはしばらくこまったようすでしたが、めをカムパネルラのほうへむけて、ではカムパネルラさん。\n",
      "Response: といいながら、そのときあしをみました。\n",
      "\n",
      "Input: となざしました。\n",
      "Response: するとあんまりいっしょに、なにかのおとこがあるいていました。\n",
      "\n",
      "Input: するとあんなにげんきにてをあげたカムパネルラが、やはりもじもじたちあったままやはりこたえができませんでした。\n",
      "Response: ブドリは、もうそのときあたまをまげていいました。\n",
      "\n",
      "Input: せんせいはいがいなようにしばらくじっとカムパネルラをみていましたが、いそいででは。\n",
      "Response: といいました。\n",
      "\n",
      "Input: よし。\n",
      "Response: わからない。\n",
      "\n",
      "Input: といいながら、じぶんでせいずをさしました。\n",
      "Response: ジョバンニは、なにかのかたちになって、そのときあたまをかけて、まっすぐにたっていました。\n",
      "\n",
      "Input: このぼんやりとしろいぎんがをおおきないいぼうえんきょうでみますと、もうたくさんのちいさなほしにみえるのです。\n",
      "Response: ジョバンニは、いっしょうけんいのち、あなたのはらをみていました。\n",
      "\n",
      "Input: ジョバンニさんそうでしょう。\n",
      "Response: ぼくはどうしてもうしました。\n",
      "\n",
      "Input: ジョバンニはまっあかになってうなずきました。\n",
      "Response: そのとき、きつねがいいました。\n",
      "\n",
      "Input: けれどもいつかジョバンニのめのなかにはなみだがいっぱいになりました。\n",
      "Response: そしてきょうきが、あかいはいってきたのです。\n",
      "\n",
      "Input: そうだぼくはしっていたのだ、もちろんカムパネルラもしっている、それはいつかカムパネルラのおとうさんのはかせのうちでカムパネルラといっしょによんだざっしのなかにあったのだ。\n",
      "Response: それからそのときは、ああ、ぼくはもうみんなはないんだ。\n",
      "\n",
      "Input: それどこでなくカムパネルラは、そのざっしをよむと、すぐおとうさんのしょさいからきょきなほんをもってきて、ぎんがというところをひろげ、まっくろなぺーじいっぱいにしろいてんてんのあるうつくしいしゃしんをふたりでいつまでもみたのでした。\n",
      "Response: そしてそのかわのみずは、ブドリは、そのとき、ちょうどそのとき、きれいなかたちになって、そのときあたまをかけて、そのにんのおとうさんが、なにかのかんがえていました。\n",
      "\n",
      "Input: せんせいはまたいいました。\n",
      "Response: おおきなこえがして、そのときぼくはまたさぶろうはまたいいました。\n",
      "\n",
      "Input: ですからもしもこのてんのかわがほんとうにかわだとかんがえるなら、そのひとつひとつのちいさなほしはみんなそのかわのそこのすなやじゃりのつぶにもあたるわけです。\n",
      "Response: おっかさんは、そのときは、ああ、ぼくはいっしょにいいました。\n",
      "\n",
      "Input: またこれをきょきなちちのながれとかんがえるならもっとてんのかわとよくにています。\n",
      "Response: そのとき、あかいはかせは、ちょっとみていました。\n",
      "\n",
      "Input: つまりそのほしはみな、ちちのなかにまるでこまかにうかんでいるあぶらあぶらのたまにもあたるのです。\n",
      "Response: そしてそのこどもは、まるでいちめんにおおきなあおじろいひかりをみていました。\n",
      "\n",
      "Input: そんならなにがそのかわのみずにあたるかといいますと、それはしんくうというひかりをあるはやさでつたえるもので、たいようやちきゅうもやっぱりそのなかにうかんでいるのです。\n",
      "Response: ところがそのこどもは、そのときは、あかいはあのあかりをあるいていきました。\n",
      "\n",
      "Input: つまりはわたしどももてんのかわのみずのなかにすんでいるわけです。\n",
      "Response: そしてそのまえにはしっていました。\n",
      "\n",
      "Input: そしてそのてんのかわのみずのなかからしほうをみると、ちょうどみずがふかいほどあおくみえるように、てんのかわのそこのふかくとおいところほどほしがたくさんつどってみえしたがってしろくぼんやりみえるのです。\n",
      "Response: ところがそのこは、おおきなこえでした。\n",
      "\n",
      "Input: このもけいをごらんなさい。\n",
      "Response: せいねんはいいました。\n",
      "\n",
      "Input: せんせいはなかにたくさんひかるすなのつぶのいっったおおきなりょうめんのとつレンズをさしました。\n",
      "Response: それはきつねがいいました。\n",
      "\n",
      "Input: てんのかわのかたちはちょうどこんななのです。\n",
      "Response: そのとき、あのかわのきしゃは、いちばんみずは、ちいさなこどものじょうに、そのときあたまをかけて、それから、そのときは、あかいのはらのおとこは、そのときのしたにはながら、かおをあかくして、そのとききしゃのおとがして、そのとき、おとうさんが、いちばんはいってき\n",
      "\n",
      "Input: このいちいちのひかるつぶがみんなわたしどものたいようとおなじようにじぶんでひかっているほしだとかんがえます。\n",
      "Response: そしてそのときは、ああ、ぼくはどうしてもうしました。\n",
      "\n",
      "Input: わたしどものたいようがこのほぼなかごろにあってちきゅうがそのすぐちかくにあるとします。\n",
      "Response: ホモイはそのときあたまをかけて、そのかわりました。\n",
      "\n",
      "Input: みなさんはよるにこのまんなかにたってこのレンズのなかをみまわすとしてごらんなさい。\n",
      "Response: せんせいはまるであいているのでした。\n",
      "\n",
      "Input: こっちのかたはレンズがうすいのでわずかのひかるつぶすなわちほししかみえないのでしょう。\n",
      "Response: ああ、こんどはまたさぶろうはすっかりきげんをなっていました。\n",
      "\n",
      "Input: こっちやこっちのかたはガラスがあついので、ひかるつぶすなわちほしがたくさんみえそのとおいのはぼうっとしろくみえるというこれがつまりきょうのぎんがのせつなのです。\n",
      "Response: それはきつねがいいました。\n",
      "\n",
      "Input: そんならこのレンズのおおきさがどれくらあるかまたそのなかのさまざまのほしについてはもうじかんですからこのつぎのりかのじかんにおはなします。\n",
      "Response: といいました。\n",
      "\n",
      "Input: ではこんにちはそのぎんがのおまつりなのですからみなさんはそとへでてよくそらをごらんなさい。\n",
      "Response: ではこんなになって、またさぶろうはすっかりおとうさんがききました。\n",
      "\n",
      "Input: ではここまでです。\n",
      "Response: ではこんなにひとりのことをいいました。\n",
      "\n",
      "Input: ほんやノートをおしまいなさい。\n",
      "Response: それから、そのとき、ちょうどそのとき、あかいはかせは、あかいはかせは、そのときあたまをかけて、いっしょうけんいのちになっていました。\n",
      "\n",
      "Input: そしてきょうしつなかはしばらくつくえのふたをあけたりしめたりほんをおもねたりするおとがいっぱいでしたがまもなくみんなはきちんとたってれいをするときょうしつをでました。\n",
      "Response: そしてそのときは、あかいはかせは、そのときあたまを、かおをあかくして、そのときあるいていきました。\n",
      "\n",
      "Input: に、かっぱんところジョバンニががっこうのもんをでるとき、おなじくのしちはちにんはいえへかえらずカムパネルラをまんなかにしてこうていのすみのさくらのきのところにあつまっていました。\n",
      "Response: それから、そのとき、きつねがいいました。\n",
      "\n",
      "Input: それはこんやのほしまつりにあおいあかりをこしらえてかわへながすからすうりをとりにいくそうだんらしかったのです。\n",
      "Response: けれども、そのときは、あかいはかせは、そのとき、そのとき、きっとみんなはいってきて、いちめんにいいました。\n",
      "\n",
      "Input: けれどもジョバンニはてをおおきくふってどしどしがっこうのもんをでてきました。\n",
      "Response: そしてそのにんは、ちょうどそのときは、あかいはかせのまえにおおきなきがして、そのにんのおとうさんが、いっぽんのきしゃをみていました。\n",
      "\n",
      "Input: するとまちのいえいえではこんやのぎんがのまつりにいちいのはのたまをつるしたりひのきのえだにあかりをつけたりいろいろしたくをしているのでした。\n",
      "Response: それから、そのまえにきて、おかあさんが、まるでいっぽんのかしわのきがみえました。\n",
      "\n",
      "Input: いえへはかえらずジョバンニがまちをみつまってあるおおきなかっぱんところにはいってすぐいりぐちのけいさんだいにいただぶだぶのしろいシャツをきたにんにおじぎをしてジョバンニはくつをぬいでのぼりますと、つきあたりのおおきなとびらをあけました。\n",
      "Response: そのとき、ちょうどそのとき、あかいはかせが、まるでいちめんにあかりました。\n",
      "\n",
      "Input: なかにはまだひるなのにでんとうがついてたくさんのりんてんうつわがばたりばたりとまわり、きれであたまをしばったりラムプシェードをかけたりしたにんたちが、なにかうたうようによんだりかぞえたりしながらたくさんはたらいていりました。\n",
      "Response: ジョバンニは、もうそのときあたまをまっあかになって、そのかわのみずのなかにはいってきました。\n",
      "\n",
      "Input: ジョバンニはすぐいりぐちからさんばんめのたかいすぐるこにすわったにんのところへいっておじぎをしました。\n",
      "Response: そのとき、きっとまたおとうさんがききました。\n",
      "\n",
      "Input: そのにんはしばらくたなをさがしてから、これだけひろっていけるかね。\n",
      "Response: といいました。\n",
      "\n",
      "Input: といいながら、いちまいのかみきれをわたしました。\n",
      "Response: そしてそのにんは、そのにんのおとうさんが、いっしょにひらいていましたが、そのとき、あかいはもうあんまりいってしまいました。\n",
      "\n",
      "Input: ジョバンニはそのにんのすぐるこのあしもとからひとつのちいさなひらたいはこをとりだしてむうのでんとうのたくさんついた、たてかけてあるかべのすみのところへしゃがみこむとちいさなピンセットでまるであわつぶぐらいのかつじをつぎからつぎとひろいはじめました。\n",
      "Response: そしてきたのです。\n",
      "\n",
      "Input: あおいむねあてをしたにんがジョバンニのうしろをとうりながら、よう、むしめがねくん、おはやう。\n",
      "Response: といいました。\n",
      "\n",
      "Input: といいますと、ちかくのしごにんのにんたちがこえもたてずこっちもむかずにれいくわらいました。\n",
      "Response: そしてそのにんは、あかいはいっていました。\n",
      "\n",
      "Input: ジョバンニはなにべんもめをぬぐいながらかつじをだんだんひろいました。\n",
      "Response: そしてそのときは、とうとうふたりのおとうさんが、いちめんにさけびました。\n",
      "\n",
      "Input: ろくときがうってしばらくたったころ、ジョバンニはひろったかつじをいっぱいにいれたひらたいはこをもういちどてにもったかみきれとひきあわせてから、さっきのすぐるこのにんへもってきました。\n",
      "Response: そしてそのまえのところへいって、そのとき、ちょうどおまえたちは、そのとき、それから、おまえたちは、そのときは、あかいはいっていました。\n",
      "\n",
      "Input: そのにんはだまってそれをうけとってかすかにうなずきました。\n",
      "Response: ジョバンニは、なにかのかんがえていました。\n",
      "\n",
      "Input: ジョバンニはおじぎをするととびらをあけてさっきのけいさんだいのところにきました。\n",
      "Response: そしてききました。\n",
      "\n",
      "Input: するとさっきのしろふくをきたにんがやっぱりだまってちいさなぎんかをひとつジョバンニにわたしました。\n",
      "Response: ホモイはまだいいました。\n",
      "\n",
      "Input: ジョバンニはにわかにかおいろがよくなっていせいよくおじぎをするとだいのしたにおいたかばんをもっておもてへとびだしました。\n",
      "Response: そしてそのこどものはなして、それから、そのかわのみずのなかにはいっているのでした。\n",
      "\n",
      "Input: それからげんきよくくちぶえをふきながらパンやへよってパンのかいをひとつとかくざとうをひとふくろかいますといちもくさんにはしりだしました。\n",
      "Response: そしてそのにんは、あおいひかりをつかまえました。\n",
      "\n",
      "Input: さん、いえジョバンニがせいよくかえってきたのは、あるうらまちのちいさないえでした。\n",
      "Response: それから、そのとき、ぼくはもうあんまりおおきなおおきなやつへはいってきて、それから、こんどはまたいっっていたのです。\n",
      "\n",
      "Input: そのみつならんだいりぐちのいちばんひだりがわにはからばこにむらさきいろのケールやアスパラガスがうえてあってちいさなふたつのまどにはにちおおいがくだりたままになっていました。\n",
      "Response: そのとき、あなたは、もうみんなはいってきたのです。\n",
      "\n",
      "Input: おかあさん。\n",
      "Response: こんにちは。\n",
      "\n",
      "Input: いまかえったよ。\n",
      "Response: いいからね。\n",
      "\n",
      "Input: ぐあいわるくなかったの。\n",
      "Response: ジョバンニはまっあかになって、いきなりそのときあるいて、いちめんのことをいってきました。\n",
      "\n",
      "Input: ジョバンニはくつをぬぎながらいいました。\n",
      "Response: そのとき、きつねがいいました。\n",
      "\n",
      "Input: ああ、ジョバンニ、おしごとがひどかったろう。\n",
      "Response: ああ、ぼくはこんなになっているんだ。\n",
      "\n",
      "Input: こんにちはすずしくてね。\n",
      "Response: それはぼくはいっしょにいいました。\n",
      "\n",
      "Input: わたしはずうっとぐあいがいいよ。\n",
      "Response: さっきのところへいってるんだ。\n",
      "\n",
      "Input: ジョバンニはげんかんをあっていきますとジョバンニのおかあさんがすぐいりぐちのしつにしろいはばをおおってねんでいたのでした。\n",
      "Response: そしてそのこどもは、いちめんになっていました。\n",
      "\n",
      "Input: ジョバンニはまどをあけました。\n",
      "Response: ぼくはそのときはきっとまっすぐにたっていました。\n",
      "\n",
      "Input: おかあさん。\n",
      "Response: こんにちはあんなにひとりがいいました。\n",
      "\n",
      "Input: こんにちはかくざとうをかってきたよ。\n",
      "Response: わたしがいいました。\n",
      "\n",
      "Input: ぎゅうにゅうにいれてあげようとおもって。\n",
      "Response: ああ、こんどはまたさぶろうはすっかりおかしながらいいました。\n",
      "\n",
      "Input: ああ、おまえさきにおあがり。\n",
      "Response: ああ、こんどはまだあるいているんだ。\n",
      "\n",
      "Input: あたしはまだほしくないんだから。\n",
      "Response: おかあさんがいいました。\n",
      "\n",
      "Input: おかあさん。\n",
      "Response: こんにちは、こういちがいいました。\n",
      "\n",
      "Input: ねえさんはいつかえったの。\n",
      "Response: ああ、こんどはまたさぶろうはまたさぶろうはすこしかおをあかくしていいました。\n",
      "\n",
      "Input: ああさんじころかえったよ。\n",
      "Response: おまえたちはいっしょうけんいのちになっているんだ。\n",
      "\n",
      "Input: みんなそこらをしてくれてね。\n",
      "Response: おかあさんは、いっしょうけんいのちになっていました。\n",
      "\n",
      "Input: おかあさんのぎゅうにゅうはきていないんだろうか。\n",
      "Response: きっとあるいていた。\n",
      "\n",
      "Input: こなかったろうかねえ。\n",
      "Response: ぼくはおまえたちはなんだかもうしろいことをいってるんだ。\n",
      "\n",
      "Input: ぼくいってとってこよう。\n",
      "Response: そしてぼくたちのほうではない。\n",
      "\n",
      "Input: あああたしはゆっくりでいいんだからおまえさきにおあがり、ねえさんがね、トマトでなにかこしらえてそこへおいていったよ。\n",
      "Response: おかあさんは、そのときあしをみました。\n",
      "\n",
      "Input: ではぼくたべよう。\n",
      "Response: ジョバンニはまだあかいひとりのことをいってきました。\n",
      "\n",
      "Input: ジョバンニはまどのところからトマトのさらをとってパンといっしょにしばらくむしゃむしゃたべました。\n",
      "Response: そしてそのまえにはなして、それからそのこどものじょうに、あかいのはらにはしっていきました。\n",
      "\n",
      "Input: ねえおかあさん。\n",
      "Response: ぼくはおまえたちはぼくはいったんだ。\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ぼくおとうさんはきっとかんもなくかえってくるとおもうよ。\n",
      "Response: ああこのときは、あのときは、あのときは、あたまをあけて、いちろうがたずねました。\n",
      "\n",
      "Input: あああたしもそうおもう。\n",
      "Response: またさぶろうはすっかりおとこはあたまをあけて、それからもうしました。\n",
      "\n",
      "Input: けれどもおまえはどうしてそうおもうの。\n",
      "Response: だいじょうぶですよ。\n",
      "\n",
      "Input: だってけさのしんぶんにこんねんはきたのほうのりょうはだいへんよかったとかいてあったよ。\n",
      "Response: それから？それから？それから？それから？それから？それから？それから？それから？それから？それから？それから？それから？それから？それから？それから？それから？それから？それから？それから？それから？それから？それから？それから？それから？それから？それか\n",
      "\n",
      "Input: ああだけどねえ、おとうさんはりょうへでていないかもしれない。\n",
      "Response: こういちはまだあおいひかりをして、それからあたまをあけて、いっしょうけんいのち、あおいひかりをして、そのときあしをたてていきました。\n",
      "\n",
      "Input: きっとでているよ。\n",
      "Response: そうだ。\n",
      "\n",
      "Input: おとうさんがかんごくへいるようなそんなわるいことをしたはずがないんだ。\n",
      "Response: このときは、あかいはたいへんあるいているんだ。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):  \n",
    "    x_in = x_encoder[i:i+1]  # 入力\n",
    "    responce = respond(x_in)  # 返答\n",
    "    print(\"Input:\", x_sentences[i])\n",
    "    print(\"Response:\", responce)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小説内の文章に対して、しばしばそれらしい返答ができていますね。  \n",
    "中には、意味がよく分からない返答もあります。\n",
    "\n",
    "次のレクチャーでは、小説外の文章に対してモデルがどのように返答するのか検証していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題\n",
    "上記のセルでbetaの値を変更し、返答の文章がどのように変化するか確かめてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
