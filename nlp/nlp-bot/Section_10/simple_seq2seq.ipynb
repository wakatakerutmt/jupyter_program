{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# シンプルなseq2seq\n",
    "最小限のSeq2Seqを構築し、時系列の変換を行います。  \n",
    "今回は、Seq2Seqを使って、sin関数の曲線をcos関数の曲線に”翻訳”します。  \n",
    "Seq2Seqの構築方法について、基礎から学んでいきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練用データの作成\n",
    "訓練用のデータを作成します。  \n",
    "今回は、sin関数の値をencoderへの入力、cos関数の値をdecoderへの入力、及び正解とします。  \n",
    "decoderへの入力は、正解から一つ後の時刻にずらします。  \n",
    "これにより、ある時刻におけるdecoderの出力が、次の時刻における入力に近づくように学習を行うことができます。  \n",
    "このような、ある時刻における正解が次の時刻の入力となる手法を**教師強制**といいます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "axis_x = np.linspace(-2*np.pi, 2*np.pi)  # -2πから2πまで\n",
    "sin_data = np.sin(axis_x)  # sin関数\n",
    "cos_data = np.cos(axis_x)  # cos関数\n",
    "\n",
    "plt.plot(axis_x, sin_data)\n",
    "plt.plot(axis_x, cos_data)\n",
    "plt.show()\n",
    "\n",
    "n_rnn = 10  # 時系列の数\n",
    "n_sample = len(axis_x)-n_rnn  # サンプル数\n",
    "x_encoder = np.zeros((n_sample, n_rnn))  # encoderの入力\n",
    "x_decoder = np.zeros((n_sample, n_rnn))  # decoderの入力\n",
    "t_decoder = np.zeros((n_sample, n_rnn))  # decoderの正解\n",
    "\n",
    "for i in range(0, n_sample):\n",
    "    x_encoder[i] = sin_data[i:i+n_rnn]\n",
    "    x_decoder[i, 1:] = cos_data[i:i+n_rnn-1]  # 一つ後の時刻にずらす。最初の値は0のまま。\n",
    "    t_decoder[i] = cos_data[i:i+n_rnn]  # 正解は、cos関数の値をそのまま入れる\n",
    "\n",
    "x_encoder = x_encoder.reshape(n_sample, n_rnn, 1)  # （サンプル数、時系列の数、入力層のニューロン数）\n",
    "x_decoder = x_decoder.reshape(n_sample, n_rnn, 1)\n",
    "t_decoder = t_decoder.reshape(n_sample, n_rnn, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seqの構築\n",
    "Kerasを使ってSeq2Seqを構築します。  \n",
    "今回は、Sequentialクラスではなく、**Modelクラス**を使います。  \n",
    "Modelクラスを使えば、複数の経路の入力を持つニューラルネットワークを構築可能で、状態を渡すことでRNN同士を接続することもできます。  \n",
    "今回は、Seq2SeqのRNN部分にはLSTMを使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, LSTM, Input\n",
    "\n",
    "n_in = 1  # 入力層のニューロン数\n",
    "n_mid = 20  # 中間層のニューロン数\n",
    "n_out = n_in  # 出力層のニューロン数\n",
    "\n",
    "encoder_input = Input(shape=(n_rnn, n_in))  # encoderの入力層\n",
    "encoder_lstm = LSTM(n_mid, return_state=True)  # return_stateをTrueにすることで、出力とともに状態（htとメモリセル）が得られる\n",
    "encoder_output, encoder_state_h, encoder_state_c = encoder_lstm(encoder_input)  # 出力、状態（ht）、状態（メモリセル）\n",
    "encoder_state = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "decoder_input = Input(shape=(n_rnn, n_in))  # decoderの入力層\n",
    "decoder_lstm = LSTM(n_mid, return_sequences=True, return_state=True)  # return_sequenceがTrueで、全ての時系列の出力が得られる\n",
    "decoder_output, _, _ = decoder_lstm(decoder_input, initial_state=encoder_state)  # encoderの状態を初期状態にする\n",
    "decoder_dense = Dense(n_out, activation='linear')\n",
    "decoder_output = decoder_dense(decoder_output)\n",
    "\n",
    "model = Model([encoder_input, decoder_input], decoder_output)  # 入力と出力を設定し、Modelクラスでモデルを作成\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n",
    "構築したSeq2Seqのモデルを使って、学習を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([x_encoder, x_decoder], t_decoder,\n",
    "                     batch_size=8,\n",
    "                     epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習の推移\n",
    "誤差の推移を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "plt.plot(np.arange(len(loss)), loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30エポックほどで誤差は収束しているようです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 予測用モデルの構築\n",
    "学習済みのオブジェクトから、encoder、decoderのモデルを個別に構築します。    \n",
    "encoderは入力を受け取って状態を返し、decoderは入力と状態を受け取って出力と状態を返すようにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoderのモデルを構築\n",
    "encoder_model = Model(encoder_input, encoder_state)  # 間にあるLSTM層は学習済み\n",
    "\n",
    "# decoderのモデルを構築\n",
    "decoder_input = Input(shape=(1, n_in))\n",
    "\n",
    "decoder_state_in_h = Input(shape=(n_mid,))\n",
    "decoder_state_in_c = Input(shape=(n_mid,))\n",
    "decoder_state_in = [decoder_state_in_h, decoder_state_in_c]\n",
    "\n",
    "decoder_output, decoder_state_h, decoder_state_c = decoder_lstm(decoder_input,  # 既存の学習済みLSTM層を使用\n",
    "                                                                 initial_state=decoder_state_in)  \n",
    "decoder_state = [decoder_state_h, decoder_state_c]\n",
    "\n",
    "decoder_output = decoder_dense(decoder_output)  # 既存の学習済み全結合層を使用\n",
    "decoder_model = Model([decoder_input] + decoder_state_in, [decoder_output] + decoder_state) # リストを+で結合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 翻訳用の関数\n",
    "入力を”翻訳”し、出力に変換するための関数を設定します。  \n",
    "decoderでは、各時刻ごとに予測を行い、出力と状態を次の時刻に渡します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_data):\n",
    "    state_value = encoder_model.predict(input_data)  # encoderから状態を取得\n",
    "    y_decoder = np.zeros((1, 1, 1))  # 出力の値\n",
    "    translated = []  # 翻訳結果\n",
    "    \n",
    "    for i in range(0, n_rnn):  # 各時刻ごとに予測を行う\n",
    "        y, h, c = decoder_model.predict([y_decoder] + state_value)  # 前の時刻の出力と状態を渡す\n",
    "        y = y[0][0][0]\n",
    "        translated.append(y)\n",
    "        y_decoder[0][0][0] = y  # 次の時刻に渡す値\n",
    "        state_value = [h, c]  # 次の時刻に渡す状態\n",
    "\n",
    "    return translated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 翻訳を実行\n",
    "sin関数の曲線を、cos関数の曲線に翻訳します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_idices = [0, 13, 26, 39]  # デモに使うデータのインデックス\n",
    "for i in demo_idices:\n",
    "    x_demo = x_encoder[i:i+1]  # 入力を一部取り出す\n",
    "    y_demo = translate(x_demo)  # 翻訳する\n",
    "    \n",
    "    plt.plot(axis_x[i:i+n_rnn], x_demo.reshape(-1), color=\"b\")  # 翻訳前（青）\n",
    "    plt.plot(axis_x[i:i+n_rnn], y_demo, color=\"r\")  # 翻訳後（赤）\n",
    "    \n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sinカーブがcosカーブに”翻訳”されていますね。  \n",
    "以上のように、Seq2Seqでは時系列データの変換を行うことができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題\n",
    "上記とは逆に、cos関数の曲線をsin関数の曲線に”翻訳”してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
